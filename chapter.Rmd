---
title: "Small Area Estimation for Crime Analysis"
abstract: "Victimization surveys provide essential information to study crime and emotions about crime, but their sampling designs only allow analyzing criminological variables at large spatial scales. Crime surveys are designed to allow producing precise direct estimates (i.e., weighted means and totals) for very large areas, but the size of samples in small areas is generally small and direct estimates produced for small geographies will generally be imprecise. Refined model-based small area estimation techniques may be used to increase the reliability of small area estimates produced from victimization surveys. Small area estimation is the term used to describe those methods designed to produce reliable estimates of parameters of interest (and associated measures of reliability) for areas for which only small or zero sample sizes are available. In 2008, the US Panel to Review the Programs of the Bureau of Justice Statistics recommended the use of small area estimation to produce subnational estimates of crime. Since then, small area estimation has been applied to study many variables of interest in criminology. This chapter introduces theory and a step-by-step exemplar study in `R` to show the utility of small area estimation to analyze crime and place. Model-based regional estimates of confidence in policing are produced from European Social Survey data. 
  \\par
  \\textbf{Keywords:} Confidence in policing, European Social Survey, crime mapping, open data, GIS"
author: |
  | David Buil-Gil
  | Department of Criminology, University of Manchester, UK
date: "19/05/2020"
output:
  pdf_document: default
  word_document: default
bibliography: refs.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(tidyr)
library(dplyr)
```

```{r Rmarkdown_setup, include=FALSE}
options(tinytex.verbose = TRUE)
```

**Full reference:** Buil-Gil, D. (2020). Small area estimation for crime analysis. In E. Groff & C. Haberman (Eds.), *The study of crime and place: A methods handbook*. Temple University Press.

**Contact details:** David Buil-Gil. G18 Humanities Bridgeford Street Building, Cathie Marsh Institute for Social Research, University of Manchester. E-mail address: *david.builgil@manchester.ac.uk*

**ORCID ID:** David Buil-Gil: 0000-0002-7549-6317.


# Introduction 

# Foundations of small area estimation

# Small area estimation applications for crime analysis

# Producing small area estimates of confidence in the police: Step-by-step example in `R`

## European Social Survey

The European Social Survey is a biannual cross-national survey designed to measure social attitudes, beliefs and behaviors. It has been conducted since 2001 in more than 35 European countries, and allows for cross-national and cross-sectional comparisons of crime-related issues such as the confidence in police services, worry about crime and crime victimization experience in the last 5 years. The ESS sample is designed to be representative of all individual residents aged 15 or older who live in private households in each participant country, regardless of their nationality, citizenship or language.

Although participant countries are responsible for producing their own national sampling designs, all counties must collow common sampling principles. Namely, respondents must be selected following strict random probability techniques at every stage, sampling frames can be individuals, households or addresses, quota sampling is not allowed, and non-responding units cannot be replaced. Moreover, every country must select at least 1,500 effective respondents (or at least 800 in participant countries with less than 2 million citizens). As a consequence, countries with very different number of residents may select similar sample sizes, and all geographical levels below countries (e.g., regions, counties, cities) are not planned by the original sampling design and record small sample sizes.

### Download European Social Survey data

ESS data can be downloaded from their website. But we can also download ESS data direcly into our `R` system using the `essurvey` package developed by @cimentada2019. This package is designed to facilitate loading ESS survey data into `R`. It allows users to select the countries and years they are interested to analyze and loading them directly in `R` If this is the first time we are using this package, we need to install it by using the `install.packages()` function.

``` {r install packs, eval = FALSE}

install.packages("essurvey") 

```

Once it is installed, we can load the package into our `R` environment using the `library()` function.

```{r load essurvey}

library(essurvey)

```

In order to access ESS data in `R`, first we need to create our own personal account in the ESS online portal. ESS users only need to registed once, and then they can have open access to ESS data as many times as they wish. We need to access the ESS website and create a new account with our personal details: [https://www.europeansocialsurvey.org/user/new](https://www.europeansocialsurvey.org/user/new). Filling the online registration form takes less than one minute, and once it is completed we will receive an email to confirm our registration process.

Once we are registered in the ESS platform, we can direcly import all ESS data into `R`. In this exercise we will download and analyze data from the 8th edition of ESS, which was published in 2016. We use the function `set_email()` from `essurvey` to save our email (the email account registered in the ESS platform) as a new environment variable, and then run the `import_rounds()` function to load ESS data from all participant countries. This may take a few seconds.

```{r read essurvey real, include=FALSE}

set_email("david.builgil@manchester.ac.uk")

ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)

```

```{r read essurvey fake, eval=FALSE}

set_email("your_email@domain.com") # change by your email

ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)

```

Now we have loaded the ESS data and we can begin exploring and analyzing it. If we want to see the data, we can use the `View()` function.


## Descriptive analyses

The ESS includes various questions that may be of interest for criminologists and crime analysts. For examples, some questions that we may be interested to analyze are:

**1.-** *"Have you or a member of your household been the victim of a burglary or assault in the last 5 years?"*
**2.-** *"How safe do you – or would you – feel walking alone in this area after dark?"*
**3.-** *"Using this card, please tell me on a score of 0-10 how much you personally trust each of the institutions I read out [...]"; "[...] the legal system" and "[...] the police".*

These measures have previously been used to study victimization, perceived safety, trust in the police, and trust in the legal system (e.g., REFS), but there are many other questions that may also be of interest for criminologists (e.g., racism, discrimination against immigrants, homophobia). We can read the whole ESS questionnaire here: [https://www.europeansocialsurvey.org/docs/round8/fieldwork/source/ESS8_source_questionnaires.pdf](https://www.europeansocialsurvey.org/docs/round8/fieldwork/source/ESS8_source_questionnaires.pdf).

In this exemplar study we will analyze ESS data about trust in police services, following previous research conducted by REFS. The variable name is `trstplc`, and it a Likert scale variable from 0 to 10, where 0 indicates the lowest level of trust, and 10 is the maximum value. We can begin by checking how this measure of trust in the police looks like. We will use the `summary()` function to obtain the summary statistics of this variable.

```{r explore trust police}

summary(ess$trstplc)

```

We see that the average score of trust in police in Europe is `r round(mean(ess$trstplc, na.rm = T), 2)`, and the median value is `r median(ess$trstplc, na.rm = T)`. We can use the same `summary()` function to compare the values of trust in police services with the citizens' trust in other social institutions, such as the legal system (variable `trstlgl`), politicians (`trtplt`), political parties (`trtprt`), the country's parliament (`trstprl`) or the United Nations (`trstun`). On average, we measures of trust in the police appear to be higher than the Europeans' trust in other key political and legal institutions.

Moreover, we can obtain some more detailed information about the citizens' trust in police services by counting the frequency of respondents that chose each score and creating a bar plot to visualize their distribution. We will use functions from the the packages `dplyr` [@wickham2020] and `ggplot2` [@wickham2020b] for this. More specifically, we use the `group_by()` function from `dplyr` to create groups of respondents based on their score of trust in police, and the functions `summarize()` and `mutate()` from the same package to save the results in two columns showing the number and proportion of respondents in each category. We save this new table in a new dataset called `trust_poli`.

```{r table trust police}

trust_poli <- ess %>%
  group_by(trstplc) %>%     # categories based on level of trust
  summarize(n = n()) %>%    # number of respondents per group
  mutate(prop = n / sum(n)) # proportion respondents per group

```

Then, we use the `ggplot()` and `geom_bar()` functions from `ggplot2` to create a bar graph of the number of responses per category. Before plotting this visualization, however, we will run the function `theme_set(theme_minimal())` to set a basic, neat theme for all our plots.

```{r plot trust police}

theme_set(theme_minimal()) # set white theme for plots

ggplot(data = trust_poli, aes(x = trstplc, y = prop)) + # set variables of interest
  geom_bar(stat="identity") +                           # plot bar graph
  ggtitle("Trust in police across European countries")  # change title

```

We see that few respondents have a low trust in the police, whereas most European citizens seem to trust their police forces quite a lot. This plot, nevertheless, may hide internal heterogeneity between European regions and countries. Based on this bar graph alone, we do not have enough information to be able to know if residents in all participant countries have the similar levels of trust in the police, or whether those respondents with very low or very high confidence in the police concentrate in some countries but not others. We will use small area estimation to produce estimates of trust in the police across European regions.

Since we are particularly interested in analyzing which regions in Europe have more and less trust in police services, and not only what is the level of trust in each area, we will produce regional estimates of the proportion of citizens who have a level of trust above the average in Europe. In other words, our estimates will show a value between 0 and 1 representing which proportion of residents have more trust in the police than the average of European citizens. For instance, a value of 0.6 in a given region would indicate that 60 percent of persons residing in this region have more trust in the police than the European average. Thus, we need to recode our variable of interest, and we will use the `mutate()` and `ifelse()` functions from `dplyr` to do so. Those respondents with a score above or equal to the mean will be given a value 1, whereas others will be assigned a value of 0. This will facilitate the interpretation of our results, but future research can explore producing estimates from the original 0-to-10 Likert scale.

``` {r recode trustpoli}

ess <- ess %>%
  # if trust is above or equal to mean, 1, 0
  mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
  filter(!is.na(trstplc)) # delete NAs

```

We can use the `group_by()` and `summarize()` functions seen above to explore how our recoded variable looks like. We can see, for example, that `r length(which(ess$trstplc == 1))` out of `r length(which(!is.na(ess$trstplc)))` (i,e., `r round((length(which(ess$trstplc == 1)) / length(which(!is.na(ess$trstplc))))*100, 2)`% of participants) have more trust in the police than the average in Europe.


## Exploring spatial data: Coverage and sample sizes

As mentioned above, the ESS sampling design is planned to allow producing reliable direct estimates at the level participant countries, but samples recorded at smaller scales (e.g., regions, cities) may be too small in some areas to allow producing direct estimates of adequate precision. We can check how big ESS sample sizes are in each region (variable `region`) using the functions `filter()`, `group_by()` and `summarize()` from `dplyr` to create a summary table in a new dataframe that we will call `sample_region`. Then, we can use the `summary()` function to print the summary statistics of area sample sizes.

```{r sample regions}

sample_region <- ess %>%
  filter(region != 99999) %>% # filter out NAs
  group_by(region) %>%        # categories based on regions
  summarize(n = n())          # calculate sample size

summary(sample_region$n)

```

The average sample size per region is `r round(mean(sample_region$n), 1)`, which is quite large but may be insufficient to produce reliable direct estimates. Moreover, there are areas with very small sample sizes (the minimum area sample size is `r min(sample_region$n)`), where we cannot simply rely of direct estimation techiques to generate estimates of adequate precision.

Moreover, we also need to consider that participant countries can decide whether they want to publish recorded data at the level of NUTS-1, NUTS-2, NUTS-3 or smaller scales. NUTS is the acronym of *Nomenclature of Territorial Units for Statistics*, and it refers to the spatial scales used by the European Union and Eurostat (the statistical office of the European Union) for policy making and statistical reporing purposes. NUTS are basically a way to organize European countries in regions and subregions. In England, for instance NUTS-1 are statistical regions, NUTS-2 are counties (and groups of districts in London), and NUTS-3 are generally unitary authorities (some grouped). Whereas some participant countries publish their data at the level of NUTS-2, others decide to report information for NUTS-1 or NUTS-3 areas. We can check which level of aggregation is published by each participant country in the ESS website: [https://www.europeansocialsurvey.org/data/multilevel/guide/essreg.html](https://www.europeansocialsurvey.org/data/multilevel/guide/essreg.html). We can also run the following lines of code and print this information directly in `R`.

```{r geo scale, eval=FALSE}

ess %>%
  group_by(regunit, cntry) %>% # group by spatial scale and country
  summarize(n = n())           # print sample size per country

```

We see that many counties publish data at the NUTS-2 level, but others participant countries publish their micro-data for NUTS-1 and NUTS-3 areas. We will aggregate data at the NUTS-2 level and produce estimates of confidence in the police at this scale (with the exception of Germany and the UK, who only publish data for NUTS-1).


### Converting spatial data into NUTS-2

In order to convert the spatial information provided by all countries into NUTS-2 geographies, we first need to load a lookup table that details which NUTS-3 areas are part of which NUTS-2. I have previously created and saved a lookup table in csv format in an open access Github repository, but similar tables are also available in other formats from the ESS platform: [https://www.europeansocialsurvey.org/data/multilevel/guide/bulk.html](https://www.europeansocialsurvey.org/data/multilevel/guide/bulk.html). In order to load the lookup table in `R`, we can use the `getURL()` functions from `RCurl` [ADD REF] and `read.csv()`.

``` {r read lookup}

library(RCurl)

URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/NUTS_lookup.csv")

lookup <- read.csv(text = URL)

```

Now, we can create a new column in the original ESS data that specifies the regions for which we aim to produce small area estimates of trust in the police. We will merge the lookup table with the original ESS data using a `left_join()` function and create a new column called `domain` which shows the NUTS-2 areas (or NUTS-1 in Germany and UK) for which we will produce estimates.


``` {r ess nuts3 in nuts2}

ess <- ess%>%
  left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
  rename(domain = nuts2) %>%                                 # rename NUTS2 variable
  mutate(domain = as.character(domain),                      # convert NUTS2 into character
         domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
  filter(!(domain == 99999))                                 # delete NAs

```

Now our data is clean and ready to be used to produce estimates of trust in the police at a regional level.


## Producing direct estimates

We will produce direct estimates based on the Horvitz-Thompson estimator [@horvitz1952], which is one of the most common approaches to produce direct estimates. It makes use of original survey data and survey weights to obtain design-unbiased estimates in each small area, but direct estimates may suffer from high variance and unreliability in those areas with small sample sizes. Moreover, estimates cannot be produced in areas with zero samples. We will produce Horvitz-Thompson estimates of the trust in police for European regions, but it is very likely than many estimators will not show adequate levels of precision. Model-based SAE approaches are needed when direct estimates are not precise enough.

In order to produce small area estimates, we will use the `sae` package [REFS]. We need to install it and load it into your `R` system.

``` {r load sae}

library(sae)

```

The Horvitz-Thompson estimator takes into account the population size in each area, and assumes that survey weights adjust our sample to the total population. Thus, we need to know how many people live in each region, and ensure that our weights adjust the sample to the population size. I have previously downloaded the population sizes from Eurostat and uploaded a clean dataset onto Github. Downloading data from sources of official statistics, such as Eurostat, usually mean having to spend some time cleaning the data and selecting those variables that adjust to our research needs. For the purpose of this exercise, I have cleaned the data and uploaded onto an online repository, but later we will also see how to load Eurostat data into our `R` environments.

``` {r read lookup}

URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/population.csv")

pop <- read.csv(text = URL)

```



``` {r dataset popsize}

pop <- pop %>%
  mutate(area = 1:n()) %>%                 # create numeric id value
  rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
  subset(select = c(1, 3, 2)) %>%          # reorder columns
  filter(domain %in% ess$domain)           # filter out areas not present in ESS

```



select from one another

``` {r select areas}
ess <- ess %>%
  filter(domain %in% pop$domain) # filter out areas not present in population dataset

```



Now we have almost all information necessary to produce our direct estimates: the variable of interest (variable `trstplc` in the `ess` dataset), the area population size (`pop2014` in `pop` dataset), and spatial information that matches in both datasets. Nevertheless, as introduced above, the Horvitz-Thompson estimator also requires the use of survey weights that adjust our sample to the population size. Given that the weights published by ESS are not designed to let respondents represent a specific number of citizens, but instead they were computed to adjust the sample to the population characteristics, we will need to recalibrate the ESS weights to the population sizes per region. We can do this by running the following lines of code:

``` {r compute weight}

ess_w_area <- ess %>%
  group_by(domain) %>%                      # create groups by region
  summarise(w_sum = sum(pspwght * pweight)) # sum weights per region

ess <- ess %>%
  left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units 
  left_join(pop, by = "domain") %>%           # merge region population sizes
  mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
         weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size

```

After a few steps, we now have all necessary information to produce our direct estimates of confidence in policing. We use the `direct()` function from `sae` to produce Horvitz-Thompson estimates in each region. It will also produce the Coefficient of Variation of each estimate, which will be used to assess the reliability of these direct estimates.

``` {r direct}

dir <- direct(y       = ess$trstplc,
              dom     = ess$area,
              sweight = ess$weight,
              domsize = pop[,2:3],
              replace = FALSE)

```


### Exploring direct estimates

Once we have produced our direct estimates of trust in the police, we can see how these look like by using some functions introduced above.

``` {r summary dir}

summary(dir$Direct) # summary statistics of direct estimates

# produce boxplot of coefficients of variation
ggplot(dir, aes(x=Domain, y=CV)) + 
  geom_boxplot() +
  ggtitle("Coefficient of Variation of direct estimates")

```

As you can see in the boxplot, the estimates of the majority of regions have a Coefficient of Variation smaller than 20%, which is a very good indicator of reliability of these estimates; but we also have a few regions with Coefficients of Variation larger than 25%. We can improve the accuracy of these estimates by using model-based small area estimation models.

For now, we can merge our direct estimates into the dataset of area-level information by using the `left_join()` function from `dplyr.

``` {r copy dir into pop}

pop <- pop %>%
  left_join(dir, by = c("area" = "Domain"))


```


## Downloading area-level covariates from Eurostat

In order to fit area-level models of trust in police and produce area-level estimates, we will need area-level covariates that are associated with our variable of interest. ADD SOME LITERATURE ON COVARIATES!!!

We can download various area-level covariates from Eurostat using the `eurostat` package [@lahti2020], which has been created by facilitate downloading data from Eurostat into `R`. Eurostat is a very large data repository that publishes large datasets of social, econonomic and demographic information for European countries and regions. We can use the `search_eurostat()` function to search predefined key words associated with variables of interest for our study. The function will return a list of all datasets including our keywords, and can then explore which of them are more suitable for our study. For example, we may want to know if education levels and crime rates are somehow associated with the regional levels of trust in the police, and thus we can search Eurostat datasets that include the words *"education"* and *"offender"*. I have done this search and found various variables of interest, but you can also try this at home and probably you will also find variables of interest for our models.

```{r find covariates}

library(eurostat)
eurostat_edu   <- search_eurostat("education") # search datasets about education
eurostat_crime <- search_eurostat("offender")  # search datasets about crime

```

Once we know the codes of the datasets we are interested to do, we can use the `get_eurostat()` function to import these into our `R` environment. For example, the dataset `edat_lfs_9918` includes information about the proportion of citizens between 15 and 64 in each NUTS-2 that have a higher education degree. We can download this dataset and see how it looks like:

```{r get higher edu}

he <- get_eurostat(id = "edat_lfs_9918")

```

If we open this file (using the `View()` function), we can see that it is includes information for many indicators, years, age groups, spatial scales and divided by sex. This is likely to be the case in all datasets imported from Eurostat, which means that we will need to spend some time wrangling these data to make sure we can attach these to our area-level direct estimates to estimate the area-level models needed to produce model-based estimates. For the purpose of this exemplar study, I have previously searched for datasets of interest, downloaded and cleaned their data, and merged all covariates into a unique dataset. We can load this dataset into `R` using the functions provided by `RCurl` package, but you can also spend some time trying to find better, more suitable covariates in the Eurostat website.



and we will use the `eurostat` package (loaded above) to download this data from Eurostat. The `demo_r_d2jan` dataset of Eurostat includes this type of information. We can use the `get_eurostat()` function to download this data into `R`. Then we will `filter()` out all that information that we are not interested to use.


## Fitting area-level models and predicting synthetic estimates

## Producing EBLUP estimates

### Mapping the confidence in police work in Europe

## Computing the Mean Squared Error of EBLUP estimates

### Plotting the Mean Squared Error of EBLUP estimates

## Model diagnostics

# Final remarks

# Author bio

David Buil-Gil is a Research Fellow at the Department of Criminology of the University of Manchester, UK, and a member of the Cathie Marsh Institute for Social Research at this same university. His research interests cover small area estimation applications in criminology, environmental criminology, crime mapping, emotions about crime, crime reporting, new methods for data collection and open data.

# References





















Crowdsourcing refers to the practise of enlisting the knowledge, experience or skills of a large number of citizens (*the 'crowd'*) to achieve a common goal or cumulative result, usually via a platform powered by online technologies, mobile phones, social media or a website [@howe2006]. Digital platforms allow recording large volumes of data in relatively little time at a small cost, and such data is often utilized for a variety of functions ranging from academic research to policy making and emergency management [@brabham2008; @goodchild2007; @hecker2019]. 

For example, during the 2007-2009 wildfires in the Santa Barbara area, California, residents shared their real-time knowledge about the location of fires and emergency shelters via various online forums and websites, which proved to be an invaluable source of information for disaster response [@goodchild2010]. Specific to research in crime and place, crowdsourcing projects have been used to understand people's experiences with crime and their perceptions about space and safety [e.g., @solymosi2018; @williams2017]. 

In this chapter we present some examples of how crowdsourced data can be used creatively for criminological research, and specifically highlight the strenghts and limitations of data produced from crowdsourcing platforms. We present a step-by-step exemplar study in `R` software [@R2020] using crowdsourced perceptions of safety in Atlanta, Georgia.


# Small area estimation

In criminological research, crowdsourcing has been primarily used to harness data about various forms of crime and antisocial behavior and to process information about citizens' perceptions and emotions about crime. Crowdsourced data provide new angles of insight into people's behaviors and perceptions, thus allowing researchers to devise new explanations of crime and perceived safety.

Public perceptions and emotions about crime have traditionally been analyzed by using surveys and interview-type qualitative approaches [see @gabriel2003; @warr2000], but these methods are costly and may be limited in their ability to capture the time and context-specific emotional reactions of fear [@castrotoledo2017]. They may also fail to record any behavioral responses to such emotions, such as avoiding certain places or situations, or acquiring alarm systems or weapons. As an alternartive, some researchers have endorsed the use crowdsourcing to record data about the specific places and times in which episodes of fear of crime are more frequent [@solymosi2018; @solymosi2020].

For example, @hamilton2011 developed a mobile phone app to record public perceptions of crime on public transportation in Melbourne, Australia. Similarly, @solymosi2015 designed an app and asked participants to report their worry about crime, which allowed authors to map the users' fear of crime across different areas of London, UK. @birendoim2016 developed a mobile app to record data about the perceptions of security of attendees at a music festival in Jerusalem, Israel. And @gomez2016 designed a collaborative web-based tool that allowed the citizens of Bogotá, Colombia, to report those areas in which they feel less safe.

People can report not only their perceived emotions, but also about things they see in their environments. For example, @solymosi2017 analyzed secondary data recorded from FixMyStreet, an online problem-reporting website, where citizens can report graffiti, broken street lights, and other signals of neighborhood disorder in London, UK.

Concerned with the effect of the built environment on people's perceptions of crime, @salesses2013 designed a website which presented people with two images from Google Street View, and asked them to choose 'which place looks safer'. This platform is called *'Place Pulse'*, and has received thousands of views, with people all over the world evaluating images of places based on their feelings of safety. Then, based on these evaluations, @salesses2013 produced a map of perceived safety in New York. It is this specific project from which we will be analyzing data later in this chapter. 

These are only a few examples, but there are many other crowdsourcing platforms that have been designed and utilized to study emotions about crime [see a review in @solymosi2020]. Moreover, open data recorded from social media and online forums [**LINK TO TWITTER CHAPTER**], which can be considered to be specific forms of crowdsourced data, enable detecting various forms of online crimes [e.g., hate speech towards minority groups; @miro2018], and even associate patterns of online communication with offline disorder and serious offences [@bendler2014; @williams2017; @williams2020].


# Strengths and weaknesses of crowdsourced data

Crowdsourced data about public perceptions of space and crime have some key strengths over data recorded from traditional survey methods. Due to the data being provided by people in real-time, using technology which can record auxiliary information such as GPS or time-stamp, besides the information people report we also get precise spatial data, information about immediate environmental variables, and other relevant information, without any additional cost to researchers or participants. However, the mode of production of crowdsourcing is also associated with certain limitations or weaknesses that, if uncontrolled, may affect the validity of such measures and the reliability and generalizability of our results [@builgil2020; @elliott2017]. 

@solymosi2020 conducted a systematic review of 27 studies utilizing or discussing the use of crowdsourcing to study perceptions and emotions about crime. Here we will summarize the key strengths and weaknesses identified in their review.


## Strengths

The most frequent strength of crowdsourcing and app-based methods identified by researchers was that these techniques allow capturing the spatial-temporal specific nature of fear of crime. Unlike traditional survey instruments, crowdsourcing data collection can be designed to generate point-level location data, and accurate-to-the-second time-stamp data with each report [@solymosi2018]. This is very beneficial for anyone carrying out crime and place research.

Another strength relevant to crime research is the ability for people to record data about the architectural features and environmental characteristics of spaces where they report [@chataway2017; @traunmueller2015], which ultimately allows to "un-erroneously associate them [perceptions about crime] with elements of the environmental backcloth such as incivilities, crime, and disorder” [@solymosi2015, p. 198]. Users can provide photos of their environments [e.g., with FixMyStreet; @solymosi2017], can be asked to evaluate photos [e.g., with Place Pulse; @salesses2013], or such data can be linked from other sources by a common information, like GPS location, to conduct on-site observation of places [e.g., with InseguridApp; @solymosi2020].

Crowdsourcing can also produce large sample sizes, often at a very low cost. @dubey2016, for example, analyzed more than 350,000 votes of perceived safety recorded from the Place Pulse platform; and @solymosi2017 analyzed more than 275,000 reports of disorder in London. These large samples are very costly to record by using traditional probability surveys. In this chapter we will illustrate how to download data about more than 1.5 million votes registered from the Place Pulse platform, and we will analyze more than 37,000 votes of perceived safety in Atlanta.


## Limitations and weaknesses

Perhaps the main weakness of data recorded from crowdsourcing is related to participants' self-selection. Probability surveys are carefully designed to select participants randomly, which means that all units in the population have equal probabilities of being chosen; whereas crowdsourcing projects harness data from non-probability samples who decide when and where to share their perceptions and emotions, and whether they want to participate at all [@elliott2017]. The mode of production of crowdsourced data increases the risk of self-selection bias, and as a consequence males and young citizens tend to be overrepresented in these data [@chataway2017], and citizens from deprived areas are generally less represented than persons from wealthy neighborhoods [@solymosi2018]. For example, @salesses2013 observed that 78.3% of participants who informed about their gender when using the Place Pulse platform were males, and @solymosi2017 highlight that only 26% of those who informed about their gender when reporting instances of disorder via FixMyStreet were females.

Even within the sample of self-selected participants there may be unequal participation. Many crowdsourcing platforms allow users to submit data multiple times. This may lead to participation inequality (or unequal participation), where "few users are responsible for most crowdsourced information, while the majority participate only a few times" [@builgil2020, p. 6]. To illustrate this, @dubey2016 show that 6,118 of the 81,730 persons who used the Place Pulse platform participated only once, while 30 users participated more than 1,000 times and the most prolific user voted 7,168 times. @solymosi2017 also show that one fourth of all FixMyStreet reports are produced by one percent of participants, and 73% of participants contribute only once.

It is not only people who may be unequally represented in these sorts of data, there may also be a bias in the places and times that do or do not feature prominently. Since users of crowdsourcing projects can decide where and when to participate, certain types of areas and times can be underrepresented. For instance, it is possible that app-based platforms fail to capture data from high-crime-density areas, since participants may avoid those places where they feel more exposed to crime [@innes2015]. The routine activities of participants are also reflected on an under-representation of data points at night, when people are less likely to be out and about [@blom2010].

Moreover, many of these projects rely on the enthusiasm of the crowd of participants, which may die down over time [@blom2010]. It is important to consider when such projects are launched, as the beginning is much more likely to see lots of active participation than later on in the project.  

Finally, there are ethical considerations that may arise from the use of crowdsourcing, which are related to the user's privacy (e.g., risk of participants' identification) but also concerns that these techniques may sensitize participant and increase their fear of crime by asking them to constantly think about crime-related risks [@jackson2015; @solymosi2020]. These last poins are not so much limitations as key concerns that all reasearchers working with crowdsourced data should keep at the forefront of their minds.


## Summary

Overall, crowdsourced data can be understood as data provided by participants from a large crowd of people through some digital platform to contribute knowledge, experience or skills towards one collaborative project [@howe2006]. Key strength of these data include: 

- precise spatial and temporal information,
- ability to collect contextual information about the environment, and
- large sample sizes often at low cost.

Key limitations to always address with such data include: 

- issues with generalizability from non-representative samples,
- unequal contribution from individuals within the sample,
- possible unequal geographical or temporal coverage, and
- initial enthusiasm of participation may die down.

It is further important to think about ethical concerns around the right to privacy of those contributing to these types of crowdsourcing platforms. 


# Crowdsourcing perceptions of safety: A step-by-step example in `R`

In order to illustrate the use of crowdsourcing in criminological research, we present an exemplar study using data recorded by the Place Pulse 2.0 platform mentioned earlier [see @salesses2013]. This section will introduce the Place Pulse project and provide annotated `R` scripts for downloading, cleaning and exploring this source of crowdsourced data. Then, we will analyze the spatial distribution of crowdsourced perceptions of space and safety in Atlanta, and illustrate with examples how to explore some of the known issues of crowdsourced data discussed above in a real-world dataset.


## The Place Pulse project

Place Pulse 2.0 was an online crowdsourcing platform designed to record data about citizens' perceptions of a variety of topics including safety, beauty, wealth, liveability, boredom and depression in urban areas. Each topic had a related question. To assess safety, two images were shown to participants, who then were asked to answer *‘Which place looks safer?’* (see Figure 1). Participants could also be asked which of the two images looked wealthier, more beautiful, more boring, livelier or more depressing, but we will focus on perceptions of space and safety in this chapter. 

The images were selected randomly from Google Street View across 56 cities from 28 countries. All images were originally taken between 2007 and 2012. All the data collected were stored on the Place Pulse open website (http://pulse.media.mit.edu/), and available to everyone there, but the platform closed in late 2019. We have been granted access to all the data recorded between May 28th 2013 and August 22nd 2019 to write this chapter. All data have also been uploaded onto an open repository with consent of the data producers.

![Figure 1: Place Pulse website](figures/Place_Pulse2.jpg)


## Download and explore Place Pulse data

We have saved all Place Pulse data (more than 1.5 million votes) in a data repository on [FigShare](https://figshare.com/articles/Place_Pulse/11859993). You can download this directly into `R` by using the `read.csv()` function. It is a large file so it may take some minutes to read in.

```{r readpp}

pp_data <- read.csv('https://ndownloader.figshare.com/files/21739137')

```

This dataset includes 17 variables, but we will only use the following variables: 

- *'X': *A unique identification code for each vote.
- *'right': *A unique identification code for the image in the right side of the pairwise comparison.
- *'left': *A unique identification code for the image in the left side of the pairwise comparison.
- *'voter_uniqueid': *A unique identification code given to each participant. 
- *'place_id_left': *A unique identification code for the city of the image in the left side of the pairwise comparison.
- *'place_id_right': *A unique identification code for the city of the image in the right side of the pairwise comparison.
- *'place_name_left': *The name of the city in the left side of the comparison.
- *'place_name_right': *The name of the city in the right side of the comparison. 
- *'choice': *Which image was selected as 'safer' (left, right, or equal).
- *'study_question': *The variable of interest (e.g., safety, wealth, beauty). 
- *'day': *Day of the data point (vote).
- *'time': *Time of the data point (vote).
- *'long_right': *Longitude for image on the right side of the comparison. 
- *'lat_right': *Latitude for image on the right side of the comparison. 
- *'long_left': *Longitude for image on the left side of the comparison. 
- *'lat_left': *Latitude for image on the left side of the comparison. 

The Place Pulse dataset has `r nrow(pp_data)` observations. Each observation is one comparison between two images. Thus, our units of analysis here are comparisons between an image on the left and an image on the right.

We can start by answering some descriptive questions, for example: which cities are most frequenty assessed within the Place Pulse platform? We know that for each row (each comparison) there are two images (`left` and `right`), and there are two columns that name the city for each image (`place_name_left`, `place_name_right`). To see how many times each city appears we can create two frequency tables (one for `place_name_left`, and one for `place_name_right`), and then join them and sum the two frequencies. We can use the `group_by()` and `summarize()` functions from `dplyr` package [@wickham2020] to achieve this:


```{r 2freqs}
library(dplyr) # load dplyr package

right_freq <- pp_data %>%
  group_by(place_name_right) %>% # categories based on cities on the right
  summarize(right_count = n()) # count number of units in each category

left_freq <- pp_data %>%
  group_by(place_name_left) %>% # categories based on cities on the left
  summarize(left_count = n()) # count number of units in each category

total_freq <- left_freq %>% 
  left_join(., right_freq, by = c("place_name_left" = "place_name_right")) %>% # merge
  mutate(total_count = left_count + right_count) # create summatory column

```

Now we can see the top 3 most common cities using the `top_n()` function:

```{r top3}

total_freq %>% top_n(3)

```

Atlanta appears to be the city with the largest number of votes. We can also check which variables (e.g., safety, beauty, wealth) were more frequently assessed by participants:

```{r topstudyquestions}

pp_data %>%
  group_by(study_question) %>% # categories based on study questions
  summarize(count = n()) %>% # count number of units in each category
  arrange(desc(count))  %>% # print in descending order
  filter(!is.na(study_question)) #remove NAs

```

We see that 'safety' was the most commonly assessed variable, with 509,961 votes in total. In this chapter we will examine reports of safety in the city of Atlanta. Before analysing the data, however, we can also examine if participants were more inclined to vote for images in the left or right part of the platform. In other words, we analyze if responses were biased by the position in which images were shown on the website. This is an important step to confirm the validity of this study instrument. In survey design, much thought and research goes into elements like the ordering of the questions. It is important that we consider crowdsourced platforms with the same care and attention. 

```{r topchoice}

pp_data %>%
  group_by(choice) %>% # categories based on vote (right, left or equal)
  summarize(count = n()) %>% # count number of units in each category
  mutate(`%` = round(count/sum(count), 3)*100) %>% # compute percentage
  top_n(3) # print 3 most frequent categories

```

The frequency of votes for left and right options is very similar; the left image wins about 43% of the time, the right one about 44% of the time, and the rest of users voted 'equal' (13%). Thus we can conclude that the position of the image on the website platform does not appear to have much of an affect on participants' votes.


## Cleaning Place Pulse data

Our city of interest, Atlanta is the capital city of the State of Georgia, United States. In 2018, its estimated papulation was close to 500,000 residents, and it is the 37th most populated city in the United States. It is also, as we have seen earlier, the city with the largest number of votes in the Place Pulse platform. There has been some considerable research looking into predictors of crime and fear of crime in this city, which can be used to interpret our findings later [see @mcnulty2000; @tester2011].

In order to analyze perception of safety in Atlanta using our crowdsourced Place Pulse data, the first step is to clean the data to make it as complete and useful as possible to answer our research questions. For example, in this case, we want to map the perceived safety of areas in Atlanta. However, the Place Pulse dataset includes reports from all over the world, which means that we will need to select a subset of votes. Moreover, we would like our unit of analysis to be the Atlanta locations, rather than each comparison. There are a few steps that we need to take to make the data look like what we need to answer our questions. Specifically, we need to do the following:

- Select only votes about safety.
- Select only those votes that contain images of Atlanta.
- For each area in Atlanta, create a score from all the votes.


### Select only votes about safety

We can use the function `filter()` from `dplyr` to create a new dataframe that includes those pairwise comparisons which relate to safety: 

```{r getsafetyq}

pp_s <- pp_data %>%
  filter(study_question == "safer") # select votes of safety

```

This has created a new dataframe, `pp_s` which contains `r nrow(pp_s)` votes about perceived safety.


### Select votes that contain images of Atlanta

Now we want to select those votes where at least one of the images is from Atlanta. Remember that there are two images in each comparison (i.e., `left` and `right`). In order to select rows where Atlanta appears in either one or the other, or both, we can use the *'or'* operator (`|`) to ensure we get all comparisons which feature Atlanta on at least one side.

```{r getatlandapp}

# select cases in which the image of the right or left is from Atlanta
pp_atl_s <- pp_s %>%
  filter(place_name_right == "Atlanta" | place_name_left == "Atlanta")

```

The new subset of cases has `r nrow(pp_atl_s)` votes about the safety of places in Atlanta.


### Duplicate comparisons in which both images are from Atlanta and create new columns for analysis

In many cases, the dataset we need to answer our research question is slightly different to the dataset we have from the crowdsourced platform. This is often the case with much secondary data analysis, where data were created for another purpose initially. This often means that much data wrangling is to be done to make the data look like what we need. 

We are interested in analyzing the proportion of 'safer' votes in each neighborhood of Atlanta. For this, we need the following information from each photo of Atlanta: 1) location (latitude and longitude) and 2) whether it won or lost (i.e., perceived as 'safer' or not) each vote.

First, we need every instance of a vote on each unique location in Atlanta. There are three possibilities for this: 

1) Image on the right of the pairwise comparison is from Atlanta, but image on the left is somewhere else. 
2) Image on the left of the pairwise comparison is from Atlanta, but image on the right is somewhere else. 
3) Both images are from Atlanta.

For the first two cases, we can easily find the coordinates of the image in Atlanta, and whether it was perceived as 'safer' or not, using conditional statements in the `if_else()` function. We will create a dataset of only these votes and remove, for now, all rows in which both images are from Atlanta. We will merge all votes together once all data have been cleaned. Further, in order to assign photographs to their neighborhood, we need to create two new columns that specify the longitude and latitude of each image of Atlanta being assessed. We will name those columns `lat_Atl` and `long_Atl`. We will also create a new column that details whether each participant voted that the image of Atlanta was 'safer' than the other photograph or not (column `win`). In order to select those votes in which one of the images is from Atlanta, we can use the function `filter()` from `dplyr`, which we have also used above, but now, instead of the *'or'* operator (`|`) we use an *'and'* operator (`&`). We also compute the win score, and save a unique ID for each image:

```{r others}

atl_v_others <- pp_atl_s %>%
    # select where only one image is from Atlanta
    filter((place_name_right == "Atlanta" & place_name_left != "Atlanta") | 
           (place_name_right != "Atlanta" & place_name_left == "Atlanta")) %>% 
    # get coordinates from Atlanta side, label win or no win, and ID of Atlanta image
    mutate(lat_Atl   = if_else(place_name_right == "Atlanta", lat_right, lat_left),
           long_Atl  = if_else(place_name_right == "Atlanta", long_right, long_left),
           win       = if_else((place_name_left == "Atlanta" & choice == "left") |
                              (place_name_right == "Atlanta" & choice == "right"), 1, 0),
           unique_id = if_else(place_name_right == "Atlanta", right, left))

```

Some pairwise comparisons, however, assessed two different images from Atlanta, which means that we will need to duplicate those votes to account for both images. For this we can create a dataset for the right side and another dataset for the left side, also creating the new latitude (`lat_Atl`), longitude (`long_Atl`), and win (`win`) columns for each using `mutate()`:

```{r duplicates}

#create dataset from Atlanta images on right side
right_side <- pp_atl_s %>%
     filter(place_name_right == "Atlanta" & place_name_left == "Atlanta") %>% 
     mutate(lat_Atl = lat_right,
           long_Atl = long_right, 
           win = if_else(choice == "right", 1, 0), 
           unique_id = right)

#create dataset from Atlanta images on left side
left_side <- pp_atl_s %>%
    filter(place_name_right == "Atlanta" & place_name_left == "Atlanta") %>% 
    mutate(lat_Atl = lat_left,
          long_Atl = long_left, 
          win = if_else(choice == "left", 1, 0), 
          unique_id = left)

```

Finally, we can merge all the data together with the `rbind()` function:

```{r merge}

pp_atl_s <- rbind(atl_v_others, left_side, right_side) 

```

We have a dataframe of `r nrow(pp_atl_s)` votes about the safety in Atlanta that is ready to be analyzed, which now fits our criteria, in that: 

- it contains only votes about safety,
- it contains only votes about Atlanta, and
- each row is a win (or no win) of a vote where an image from Atlanta was chosen as safer (or not safer).

We can now take this dataset and draw some conclusions.


## Environmental correlates of safety

Researchers may be interested in analyzing the environmental characteristics of those places assessed by Place Pulse users as safe or not safe. From such data we can get the coordinates of places rated, and use tools such as Google Street View to conduct a virtual environmental audit of these places. For example, we can look at the safest and least safe rated places using our dataset in Atlanta. For this, we need a dataset where each unique image is our unit of analysis, and we can compute a win score by considering the proportion of votes which that image has won: 

```{r image_safest}

pp_atl_s %>% 
  group_by(unique_id) %>% # groups by images IDs
  summarize(winscore = mean(win, na.rm = TRUE), # calculate proportion of 'safer'
           num_votes = n(),  # count number of votes
           latitude = first(lat_Atl), longitude = first(long_Atl)) %>% # save lat, long
  filter(num_votes >= 25) %>% # filter out pictures with less than 25 votes
  arrange(desc(winscore)) %>% # order by proportion of 'safer' responses
  filter(row_number() == 1 | row_number() == n()) # print highest and lowest score

```

This may be used by criminologists to observe the characteristics of those places assessed by Place Pulse participants as more or less safe. For example, we can use Google Street View [[https://www.google.com/maps](https://www.google.com/maps)] to observe places rated as the least safe or safest amongst those rated at least 25 times (see Figure 2). In this case, the least safe place is characterized by signs of physical disorder (i.e., graffiti, abandoned card, rubbish lying around) which may increase negative emotions about crime [see @toet2012]. Moreover, there are corners and hidden spaces between abandoned cars that be perceived to offer concealment for possible criminals and obstructs the view onto certain spaces [see @fisher1992]; whereas the safest place is a wide street of a well-maintained residential area with green spaces, direct visual access to most places around it (large prospect), and natural surveillance from the house [@welsh2004]. We can do much more, but here we will focus on the specific issues to explore due to the crowdsourced nature of these data.

![Figure 2: Least safe and safest places rated by at least 25 Place Pulse users](figures/unsafe_safe25.jpg)


## Mapping Place Pulse data

We can use mapping techniques learned in other chapters (**LINK WITH MAPPING CHAPTERS**) to map crowdsourced data. We will be using the `sf` [@pebesma2020] and `ggplot2` [@wickham2020b] libraries in order to create a map of perceived safety of built environment across the areas of Atlanta.

First, acquire a polygon which represents the census tracts of Atlanta. The Georgia Association of Regional Commission, for instance, publishes spatial data for Atlanta Region at the different spatial scales. We can go on their website to find out more about this boundary data:[https://opendata.atlantaregional.com/datasets/census-2000-tracts-atlanta-region](https://opendata.atlantaregional.com/datasets/census-2000-tracts-atlanta-region). We can download the shapefile directly using their Application Programme Interface (or API) and the `st_read()` function form the `sf` package:

```{r loadshapefile}

library(sf)

# download geojson from Georgia Association of Regional Commissions open data
atl <- st_read("https://opendata.arcgis.com/datasets/04b79404794f43959cda4f8c3f1817e6_49.geojson")

```

Note: if this link does not work, we saved a local copy of this file, in that case you can download from the following link: [https://raw.githubusercontent.com/maczokni/crowdsourcing_pp_chapter/master/geojson/Census_2000_Tracts_Atlanta_Region.geojson](https://raw.githubusercontent.com/maczokni/crowdsourcing_pp_chapter/master/geojson/Census_2000_Tracts_Atlanta_Region.geojson).

We can see what this file looks like by using the `plot()` and `st_geomerty()` functions to plot the geometry of the 'atl' object we created: 

```{r plotatlgeom}

plot(st_geometry(atl),
     main = "Atlanta Region census tracts")

```

In order to plot the safety votes on this map, we first need to make our votes a spatial object, by specifying that the *'long_Atl'* and *'lat_Atl'* columns contain our longitude and latitude information. We use the `st_as_sf()` function for this: 

```{r geocodepp}

points_atl_s <- st_as_sf(pp_atl_s, coords = c("lat_Atl", "long_Atl")) #geocode votes

```

In order to plot both these spatial layers (i.e., votes recorded from Place Pulse and Atlanta census tracts) on the same map, their coordinate reference systems (CRS) need to match. We can assign the CRS from our polygon to our points layer: 

```{r changecrs}

st_crs(points_atl_s) <- st_crs(atl)

```

If we check, they should have the same CRS: 

```{r checkcrs2}

st_crs(points_atl_s) == st_crs(atl) #check if CRS is the same in both layers

```

Now, to map 'safer' votes per census tract, we will compute a proportion of wins in each tract that will allow us to directly analyze the geographical distribution of perceived safety [see @builgil2020]. 

```{r calcwinscore}

points_atl_s_nhood <- st_intersection(atl, points_atl_s) %>% # intersection of points and areas
                      group_by(TRACT) %>% # make groups based on tracts
                      summarise(winscore = mean(win, na.rm = TRUE), # proportion 'safer'
                                num_votes = n()) %>%  # count number of votes
                      st_set_geometry(NULL) #remove geometry (so we can join to polygon layer)

```

We have a dataframe which has the proportion of wins (`winscore`) for each tract. All that is left is to join this to our polygon of Atlanta (`atl`) and produce a map. 

```{r joinwin}

# merge census tracts and Place Pulse votes based on common 'TRACT' column
atl_pp_wins <- left_join(atl, points_atl_s_nhood, by = c("TRACT" = "TRACT")) %>%
  filter(!is.na(winscore)) # delete census tracts with 0 votes (NAs)
  
```

Finally, map the propotion of 'safer' votes in each census tract.

```{r mapwins}

ggplot(data = atl_pp_wins) + 
  ggtitle("Proportion of 'safer' votes per census tract") +
  geom_sf(aes(fill = winscore)) +
  coord_sf(xlim = c(-84.7, -84), ylim = c(33.5, 34), expand = FALSE) +
  theme_void() 

```

Areas in the south part of the city appear to have lower levels of perceived safety, whereas most areas in North Atlanta have higher values of perceived safety.


## Exploring known issues of crowdsourced data within Place Pulse

As we described earlier in the chapter, crowdsourced data comes with many possible issues which need to be properly understood. Here we illustrate how to explore some of these issues, working through examples from Place Pulse. 


### How representative is the sample? 

Place Pulse data contains votes from a self-selected sample, and unfortunately, the Place Pulse project did not record information about participants' demographic characteristics. Thus, we cannot direclty examine the self-selection biases that may affect this dataset [@elliott2017; @chataway2017]. However, the sample's self-selection bias should be checked when possible. In this case, we do know that the first edition of Place Pulse did record some demographic variables from participants, and that in that iteration, 78.3% of participants were males, and only 21.7% were females, and the median self-reported age was 28 years [@salesses2013]. We can expect that the version of Place Pulse we explore may have similar characteristics.


### Participation inequality (*'supercontributors'*)

Another issue is the participation inequality within the sample. Crowdsourced data tend to be affected by a few number of supercontributors that produce most votes [@dubey2016; @solymosi2017]. In order to check if our dataset is affected by this, we can use the variable `voter_uniqueid` and produce a frequency table:

```{r create voter table}

voter <- pp_data %>% 
  group_by(voter_uniqueid) %>% # create groups based on users unique id
  summarise(num_votes = n()) # print the number of votes by user

```

We can have a look at this new dataframe using the `View()` function, and see that we have some *very active* participants. The top participant, for instance, has made `r voter %>% arrange(-num_votes) %>% head(1) %>% pull(num_votes)` votes on places. That is some very prolific participation. On the other hand, we can also see that `r nrow(voter %>% filter(num_votes == 1))` of the participants made only one vote. We are definitely seeing signs of participation inequality in these data. 

In fact, we can examine how many votes are produced by these 'supercontributors'. For example, we can assess the proportion of votes made by the top 1% of voters. We can do this using the `subset()` and `quantile()` functions:

```{r top1perc}

# subset top 1% of most prolific participants
top_1percent <- subset(voter, num_votes > quantile(num_votes, prob = 1 - 1/100)) 

```

We see that this new dataframe contains `r nrow(top_1percent)` people, who are our top 1% contributors to the Place Pulse dataset. We will now examine how much of the total number of votes are generated by the top 1% of users:

```{r top1perccontrib}

# proportion of votes by top 1% participants
sum(top_1percent$num_votes) / sum(voter$num_votes) * 100

```

```{r activity1, echo = FALSE, eval = FALSE}

top_10percent <- subset(voter, num_votes > quantile(num_votes, prob = 1 - 10/100)) #subset top 10% participants

sum(top_10percent$num_votes) / sum(voter$num_votes) * 100 #Proportion of votes by top 10% participants

top_25percent <- subset(voter, num_votes > quantile(num_votes, prob = 1 - 25/100)) #subset top 25% participants

sum(top_25percent$num_votes) / sum(voter$num_votes) * 100 #Proportion of votes by top 25% participants

```

That is a lot: `r round(sum(top_1percent$num_votes) / sum(voter$num_votes) * 100, 2)`% of the votes are made by the top 1% of contributors. We can also compute the proportion of votes made by the top 10% and 25% of participants in the same way. The top 10% contributors are responsible for 46.06% of votes, and the top 25% users contribute the 66.20% of all votes.


### Quantifying participation inequality

One way to quantify the extent to which participation inequality exists in our data is by using a Gini index, and visualizing it using a Lorenz curve. The Gini index (or Gini ratio) is a measure of statistical dispersion intended to measure inequality [@gastwirth1972]. Although it is generally used to examine income inequality, it has also been frequently used to assess participation inequality in crowdsourcing platforms [see @solymosi2017; @solymosi2018]. Similarly, the Lorenz curve is a visual representation of inequality. For this we will need the `ineq` library [@zeileis2015]. We can load this library and calculate the index using the `Gini()` function applied to our 'number of votes' column in our frequency table dataframe: 

```{r gini}

library(ineq)

Gini(voter$num_votes) # print Gini index

```

To interpret this number, we can consider the following. A Gini index score of 0 represents perfect equality (everyone makes equal number of votes), while 1 shows perfect inequality (only one person making every single vote). Our answer of `r round(Gini(voter$num_votes), 2)` shows some serious inequality. To put this into context, in 2017, according to the OECD, income inequality in the United States showed a Gini coefficient of 0.39. To visualize this we can use a Lorenz curve using the `plot()` and `Lc()` functions: 

```{r lorenz}

plot(Lc(voter$num_votes), # plot Lorenz curve
     xlab = "Cumulative share of participants from lowest to higher number of votes",
     ylab = "Cumulative share of votes", col = "darkred", lwd = 2) 

```

The Lorenz curve (red line) shows how the top few percent of users contribute the majority of the reports. If we had perfect equality, we would expect to see the red line align perfectly with the black line with the slope of 1. With this information we can now quantify how severe the participation inequality is in our data, and compare with other crowdsourced data for context and understanding.


### Under-representation of certain areas

Another important consideration is the variation in the sample size across different areas. Some places might have many votes, while others not so much. While Place Pulse might not suffer from underreporting of high-crime-density areas due to people avoiding them (people are randomly presented with images from all over), it is still important to consider the number of votes in each census tract. We can analyze if certain areas are under-represented in our dataset by using the `summary()` function applied to the number of votes (`num_votes`) variable in our polygon object from earlier (`atl_pp_wins`).

```{r samplesize}

summary(atl_pp_wins$num_votes)

```

Whereas the average sample size per area is quite large (`r round(mean(atl_pp_wins$num_votes), 2)`), some tracts are clearly over-represented (the maximum number of votes is `r round(max(atl_pp_wins$num_votes), 2)`) and others suffer from small representation (the minimum number of votes is only `r round(min(atl_pp_wins$num_votes), 2)`).

We may also want to know which areas suffer from severe under-representation in our dataset. We use the function `mutate()` to compute the number of votes divided by square miles in each census tract.

```{r votes_per_sqmile}

# compute new column of number of votes divded by square miles
atl_pp_wins <- atl_pp_wins %>%
  mutate(votes_sqmile = num_votes / SQ_MILES)

```

Then we can visualize the geographic distribution of the number of votes per census tract using the same code shown above to map perceptions of safety.

```{r map_votesmile}

ggplot(data = atl_pp_wins) + 
   ggtitle("Number of votes per square mile") + 
   geom_sf(aes(fill = votes_sqmile)) + 
   coord_sf(xlim = c(-84.7, -84), ylim = c(33.5, 34), expand = FALSE) + 
   theme_void()

```

We see that areas in the city center tend to have larger number of votes per square mile and are therefore well represented, whereas tracts in surrounding areas suffer from smaller sample sizes. Estimates of perceived safety in under-represented areas are likely to be affected by a small number of responses and may suffer from low precision. In order to increase the reliability of estimates produced from crowdsourced data for areas with small sample sizes, some researchers suggest using resampling and model-based techniques [see @arbia2018; @builgil2020].


### Participation decrease

Finally, some researchers have identified that the number of users of crowdsourcing projects decreases over time: whereas the number of participants tends to be large during the first few days, users lose interest in the project if they do not obtain clear short-term benefits from using it [see @blom2010; @solymosi2020]. We can also expore whether our Place Pulse dataset is affected by participation decrease.

We will use various functions from `dplyr` and `ggplot2` packages (seen above) to visualize the number of votes between the Place Pulse website launch and its closure. But we also need to use other key functions: 1) the `ymd()` function from `lubricate` package is used to transform the dates in which votes took place into `Date` objects [@spinu2020], and 2) the `complete()` function from `tidyr` package is used to turn implicit missing dates into explicit missing dates and create a timeline of dates with and without votes [@wickham2020c].

```{r datestuff}

by_day <- pp_data %>% 
          mutate(day = ymd(day)) %>% # create column by transforming into 'Date' object
          group_by(day) %>% # create groups by days
          summarise(num_votes = n()) %>% # count number of votes by day
          complete(day = seq.Date(min(day), max(day), by = "day")) %>% # complete all days
          mutate(num_votes = replace_na(num_votes, 0)) # create new column: votes by day

ggplot(by_day, aes(x = day, y = num_votes)) + 
       geom_line() + 
       geom_smooth(lwd = 1.5, col = "red") + 
       theme_bw() + 
       xlab("Days since website launch") + 
       ylab("Number of votes") 

```

The number of votes within the Place Pulse platform clearly decreased over time, but we also observe some peaks even years after the launching of the project. Some of these peaks match the dates of key publications using Place Pulse data and media releases, which shows that participation in crowdsoucing projects can be enhanced by periodic campaigns. For example, we observe a large peak beginning on July 24th 2013, date in which @salesses2013 published their paper and the Massachusetts Institute of Technology published a news article about the Place Pulse platform on their website: [http://news.mit.edu/2013/quantifying-urban-perceptions-0724](http://news.mit.edu/2013/quantifying-urban-perceptions-0724). We also observe another peak of participation beginning on October 15th 2014, just after the publication of @harvey2014 Master's thesis about how to automate the study of the characteristics of streetscape skeletons and urban perceptions from Place Pulse data.


# Conclusions

The open data movement has provoked a revolution in social research methods, and will continue changing the way in which many social issues are researched, understood and managed. Digital technologies enable large volumes of data to become available for social researchers and data scientists, and crowdsourcing is becoming a key source of data to analyze and map social phenomena such as crime [@bendler2014] and perceptions of space and safety [@solymosi2015; @solymosi2018]. In this chapter we have described and explored the main strengths and weaknesses of using crowdsourced data for criminological research. Specifically, we have obtained access to a large dataset of more than 1.5 million votes about urban perceptions recorded from the Place Pulse project [@salesses2013], selected a sample of more than 37,000 votes of perceived safety for Atlanta, and studied the spatial distribution of perceptions of space and safety at a census tract level in this city. We have also shown how these data can be utilized to identify places assessed by participants as very safe or very unsafe; places in which researchers can then conduct observation to study those environmental features that make citizens feel fear of crime [@toet2012]. 

Although crowdsourcing offers advantages over traditional survey methods to study perceptions and emotions about crime, data recorded from crowdsourcing is also affected by certain issues that, if uncontrolled, are likely to affect the validity of data and the reliability and generalizability of research outputs. For instance, we have observed how Place Pulse votes are largely produced by a few number of super-contributors (i.e., participation inequality), there is under-representation of certain areas outside the city center, and the number of votes decreases over time (i.e., participation decrease). These issues have also been observed in data produced from many other crowdsourcing and app-based projects [e.g., @chataway2017; @solymosi2015; @solymosi2020; @traunmueller2015]. Other researchers have also highlighted that crowdsourced data tends to be affected by self-selection bias, which explains why males tend to participate more than females, and young persons more than adults [@salesses2013; @solymosi2018]; but the Place Pulse platform did not record demographic variables from participants and we have not directly assessed this issue here.

Due to the fact that crowdsourced datasets - and non-probability samples in general - may be affected by these potential sources of unrepresentativeness and bias, several researchers are exploring new techniques to enable obtaininig reliable research outputs. @elliott2017, for example, present different methods to compute individual pseudo-sampling weights and adjust non-probability samples to target populations; @arbia2018 have developed a method to delete spatial outliers and calculate weights to adjust non-probability samples to optimal spatial samples; and @builgil2020 investigate the use of resampling and model-based small area estimation techniques to allow producing reliable estimates at detailed spatial scales from crowdsourced data. Academics and practitioners will benefit from methods to mitigate the sources of bias in crowdsourced data, which may allow obtaining more precise and reliable - but also cheaper - findings and devise new explanations of crime, antisocial behavior and emotions about crime. In the context of crime analysis, bias-corrected crowdsourced data may become a key tool to understand crime patterns, anticipate crime trends and even provide assistance to police investigations [@bendler2014; @nhan2017].


# Authors bios

David Buil-Gil is a Research Fellow at the Department of Criminology of the University of Manchester, UK, and a member of the Cathie Marsh Institute for Social Research at this same university. His research interests cover small area estimation applications in criminology, environmental criminology, crime mapping, emotions about crime, crime reporting, new methods for data collection and open data.

Reka Solymosi is a Lecturer in Quantiative Methods at the Department of Criminology of the University of Manchetser, UK, with interests in data analysis and visualization, crowdsourcing, rstats, fear of crime, transport, and collecting data about everyday life. As a former crime analyst, she is interested in practical applications to research and solving everyday problems with data.


# Acknowledgments

The authors would like to thank César A. Hidalgo for providing access to the data used in this book chapter.


# References