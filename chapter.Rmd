---
title: "Small Area Estimation for Crime Analysis"
abstract: "Victimization surveys provide key information about crime, perceived safety and trust in the police, but these surveys are only designed to allow aggregating responses at large spatial scales. The design of most victimization surveys permits producing precise direct estimates (i.e., weighted means and totals) for very large areas, such as countries or states, but sample sizes in smaller areas are generally very small and direct estimates produced at small spatial scales tend to be imprecise and unreliable. Refined model-based small area estimation techniques may be used to increase the reliability of small area estimates produced from victimization surveys. Small area estimation is the term used to describe those methods designed to produce reliable estimates of parameters of interest (and their associated measures of reliability) for areas for which only small or zero sample sizes are available. In 2008, the US Panel to Review the Programs of the Bureau of Justice Statistics recommended the use of small area estimation to produce subnational estimates of crime. Since then, small area estimation has been applied to study many variables of interest in criminology. This chapter introduces theory and a step-by-step exemplar study in `R` to show the utility of small area estimation to analyze crime and place. Small area estimates of trust in the police are produced from European Social Survey data. 
  \\par
  \\textbf{Keywords:} Confidence in policing, European Social Survey, crime mapping, open data, GIS"
author: |
  | David Buil-Gil
  | Department of Criminology, University of Manchester, UK
date: "01/06/2020"
output:
  pdf_document:
        latex_engine: xelatex
  word_document: default
bibliography: refs.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
```

```{r Rmarkdown_setup, include=FALSE}
options(tinytex.verbose = TRUE)
options(xtable.comment = FALSE)
```

**Full reference:** Buil-Gil, D. (2020). Small area estimation for crime analysis. In E. Groff & C. Haberman (Eds.), *The study of crime and place: A methods handbook*. Temple University Press.

**Contact details:** David Buil-Gil. G18 Humanities Bridgeford Street Building, Cathie Marsh Institute for Social Research, University of Manchester. E-mail address: *david.builgil@manchester.ac.uk*

**ORCID ID:** David Buil-Gil: 0000-0002-7549-6317.


# Introduction 

The study of crime and place is moving towards the study of smaller levels of geography than ever before, to foreground those places where crime concentrates, and to develop spatially targeted strategies to tackle crime and disorder [@braga2018; @groff2010; @weisburd2012]. The move towards micro-level crime mapping has also provoked that researchers preoccupied with the study of emotions about crime and perceptions about the police begin shifting their attention towards to study of small levels of analysis, to study the effect of the immediate environment and social context on people's perceptions and emotions about crime and the police [e.g., @solymosi2015; @williams2019]. These issues have become topics of major interest for researchers, crime analysts and police administrations, who require precise - and reliable - maps of their spatial distribution to develop more accurate theoretical explanations and more efficient evidence-based policing strategies.

In order to visualize those micro places where crime is more prevalent, police-recorded crimes and calls for police services are the main source of data used by academics and police forces. Crimes known to police, however, may be affected by missing data due to underreporting affecting some areas more than others [@brantingham2018; @xie2019]. Victimization surveys provide key information to account for crimes known and unknown to police, and are the main source of information to analyze emotions about crime and perceptions about the police [@rosenbaum1995; @skogan1977]. The two main victimization surveys are probably the National Crime Victimization Survey (NCVS) in the United States, and the Crime Survey for England and Wales in the UK. However, these crime surveys are not designed to allow researchers and practitioners to aggregate responses at the level of small geographies. Sample sizes are large enough to produce direct outputs only for large spatial scales, such as countries or states, but small levels of geography suffer from small sample sizes. In more precise terms, surveys' sampling designs are (generally) planned to enable producing precise direct estimates (i.e., weighted means or totals) for those large areas planned by survey administrators, but directly aggregating data from a few respondents residing in each small area will inevitably lead to imprecise and unreliable maps. Refined model-based small area estimation techniques (SAE) may be applied to increase the reliability of small area estimates of parameters of criminological interest produced from survey data [@rao2015].

This chapter introduces theory and a step-by-step exemplar study in `R` [@R2020] to show the utility of SAE to analyze crime and place. In the exemplar study we will illustrate how to produce small area estimates of trust in the police from European Social Survey (ESS) data.


# Foundations of small area estimation

@pfeffermann2013 defines SAE as those techniques designed to "produce reliable estimates of characteristics of interest such as means, counts, quantiles, etcetera, for areas or domains for which only small samples or no samples are available” (p. 40). SAE is an umbrella term that is used to classify many different groups of techniques developed to deal with different sets of data, but all aimed to improve the reliability of estimates produced for areas with small sample sizes [see a review of the main SAE techniques in @rao2015]. In this regard, those who use SAE techniques use the term *'small area'* to describe not only low level geographies, but any area whose sample size is too small to allow producing direct estimates of adequate precision. In the example shown below, for example, we will produce small area estimates of trust in the police in European regions. These are not small units of analysis, but suffer from small sample sizes.


## Direct estimation

As described above, direct estimators use only survey data recorded in each area, including the measure of the variable of interest and survey weights, to obtain weighted means or totals in each area. Survey weights are calculated and included in most surveys to adjust selected samples to the target population, thus making survey data more representative. We will call *'direct estimates'* those estimates computed only from survey data. Direct estimates will be unreliable in all those areas where only a small sample was recorded by the original survey. We will use the Coefficient of Variation (CV) of all units sampled in each area, which in essence is the ratio of the standard deviation to the mean, to define how *reliable* each direct estimates computed in each area is. Direct estimates may not be reliable enough in many areas, and thus we will need to apply *indirect* small area estimation techniques to calculate more reliable estimates. 


## Model-based estimation

Indirect SAE is also known as *'model-based'* SAE, since these techniques make use of explicit linking models to 'borrow strength' across all related areas. In essence, these techniques link information recorded by the original survey in each area to auxiliary variables registered by (generally) administrative agencies, and use this information to increase the reliability of our estimates in all areas. For example, if we know that the chances of falling victim to crime are associated with the level of deprivation of the area in which you reside, we can estimate a model that links the survey respondents' reported victimization to known poverty measures in each place, thus producing higher estimates of crime victimization in deprived areas than more wealthy neighborhoods. These auxiliary variables are generally called *covariates*. The quality of the covariates used and the accuracy of the linking models will be key to obtain reliable small area estimates.

When we use explicit models to link information available for all sampled units with unit-specific covariates, then we are using a *'unit-level'* SAE approaches. On the other hand, if our linking model is estimated by relating the area-level direct estimates of our variable of interest to area-level covariates, then we would be using *'area-level'* SAE. Area-level SAE approaches are generally preferred and produce more reliable small area estimates when our variable of interest is particularly affected by the contextual characteristics of each area [@namazirad2015]. The study of crime and place has shown us that crime-related variables are strongly associated with the characteristics of small areas [e.g., @bruntonsmith2012; @weisburd2012], and thus here we focus on the use of area-level SAE.


### Regression-synthetic estimation

More specifically, when some sort of regression model is used to link survey data to covariates in order to estimate regression coefficients (and sometimes other model parameters) and compute area-level predictions from these, then we are producing regression-based synthetic estimators. These can be based on linear, logistic, multilevel or more complex modeling approaches. Synthetic estimates can be calculated for all areas, but these are too model dependent and susceptible to model misspecification [@levy1979; @rao2015]. Synthetic estimates suffer from a high risk of bias. We will see illustrate how to calculate area-level regression-based synthetic estimates later on, and we will also show why these may be insufficient due to their high risk of bias.


### The Empirical Best Linear Unbiased Predictor (EBLUP)

A composite estimator that combines the direct and synthetic estimate in each is the preferred approach to rectify all these deficiencies. The basic SAE approaches consist precisely of optimal combinations between two components given by the direct estimate, which will be more reliable in some areas than others, and the synthetic estimate. Since sample sizes will generally be larger in some areas than others, calculating direct estimates from survey data will generate direct estimates in with unequal levels of reliability. What these composite estimators do is give more weight to the direct estimate when its sampling variable is small, while more weight is attached to the synthetic estimator when the variance of the direct estimate becomes larger. One of the most widespread area-level composite estimators in SAE is the EBLUP developed originally by @fay1979. In this chapter we will produce EBLUP estimates of trust in the police in Europe.


## Calculating the estimates' uncertainty

One of the main advantages of using SAE is that a great deal of work has been carried about how to estimate the measure of uncertainty of each small area estimate produced in each area. These measures of uncertainty are usually presented as Mean Squared Errors (MSE) or Relative Root Mean Squared Errors (RRMSE). The MSE is the averaged squared error of an estimate, representing the difference between the estimate value and the expected true value of what is measured. MSE is computed to account for both the variance of estimates (i.e., spread of estimates from one sample to another) and their bias (i.e., distance between the estimated value and the true value). The MSE is always non-negative, and values closer to zero indicate a higher level of reliability of our estimate. We obtain the RRMSE by taking the square root of the MSE (i.e., the calculate the Root Mean Squared Error, RMSE), and then dividing it by the corresponding estimate. RRMSEs are generally presented as percentages.

Calculating the RRMSE of estimates allows examining which small area estimation methods produces the most reliable estimates, but also foregrounding which specific estimate in which specific area may suffer from inadequate reliability. Different statistical agencies use different threshold points to consider that an estimate is reliable enough. Here we consider that estimates with a RRMSE smaller than 25% is reliable, estimates with a RRMSE between 25% and 50% can be used with caution, and estimates with a RRMSE larger than 50% is considered too unreliable to be used [@commonwealth2015].

We will use the CV of direct estimates as their measure of uncertainty, since it corresponds to the RRMSE of model-based estimators [@rao2015]. To compute the RRMSE of our EBLUP estimates with we follow the analytical procedure described in @datta2000.


# Small area estimation applications for crime analysis

SAE may be of great value for the study of crime and place: to estimate the geographical distribution of crimes known and unknown to police and to produce detailed maps of crime-related perceptions and emotions. This is the reason why, in 2008, the US Panel to Review the Programs of the Bureau of Justice Statistics (BJS) recommended the use of model-based SAE to produce subnational estimates of crime rates: “BJS should investigate the use of modelling NCVS data to construct and disseminate subnational estimates of crime and victimization rates” [@groves2008, p. 8]. This work was started by Robert E. Fay and colleagues at the BJS to produce estimates of victimization rates for states and large counties in the US [@fay2012; @fay2015]. The need to apply SAE to estimate crime in places has also been acknowledged by the Australian Bureau of Statistics [@tanton2001] and Statistics Netherlands [@buelens2009].

Some researchers have also applied different regression-based synthetic estimators to produce small area estimates of crime and disorder, but, as discussed above, these are known to suffer from a high risk of producing biased estimates due to model misspecification [@levy1979; @rao2015]. We will see illustrate how to produce simple regression-based estimates later on, and show why these may be insufficient due to their high risk of bias.

Others have used the basic unit-level or area-level EBLUP, or the temporal extensions of the area-level EBLUP, to produce estimates of crime rates. @buelens2009 used the area-level EBLUP based on Fay-Herriot model [@fay1979] to produce estimates of victimisation rates in police zones in the Netherlands. Fay and colleagues developed the area-level dynamic SAE model and produced estimates of crime rates in states and large counties in the US [@fay2012; @fay2015]. @dalo2012 made use of the basic unit-level and area-level EBLUP models to produce estimates of rates of violence against women at a regional level in Italy. And @builgil2019b and @builgil2019c applied spatial extension of the area-level EBLUP to produce estimates of worry about crime in Europe and perceived neighborhood disorder in Manchester, respectively. Some researchers have also used Bayesian approaches to estimate victimization rates and confidence in the police [e.g., @brakel2014; @law2014; @williams2019]. Here we will show to produce basic area-level small area estimates of trust in the police in Europe.


# Small area estimation of trust in the police: Step-by-step example in `R`

In this section we will demonstrate how to produce small area estimates with real world data. More specifically, we will use data recorded by the ESS to produce estimates of trust in the police across regions in Europe. In practice, crime-and-place researchers will be particularly interested in applying SAE techniques to estimate crime victimization, perceived safety and trust in the police at the scales of much smaller levels of analysis, but survey data with such level of spatial granularity are usually subject to great levels of scrutiny and survey administrators tend not to publish micro-data at the level of small geographies. Access to such data is generally subject to special permissions and cannot be shared openly. The example shown here, however, will illustrate how to apply SAE methods to a variety of datasets and variables of interest.

The exemplar study is designed for you to acquire a number of different practical skills, which include:
• Downloading and exploring survey data.
• Analyzing issues around survey data coverage of small areas and small sample sizes.
• Producing direct estimates for small areas and exploring their reliability.
• Producing area-level small area estimates, which compromise several steps:
– obtaining access to reliable area-level covariates,
- estimating area-level regression models and predicting synthetic estimates,
- calculating EBLUP estimates and their RRMSE, 
- visualizing the spatial distribution of EBLUP estimates, 
- visualizing the improved reliability of EBLUP estimates over direct estimates, and
- producing model diagnostics check that our model-based estimates are model-unbiased.


## European Social Survey

As discussed above, in this exemplar study we will use ESS data. The ESS is a biannual cross-national survey designed to measure social attitudes, beliefs and behaviors in Europe. It has been conducted since 2001 in more than 35 countries, and is designed to be representative of all individual residents aged 15 or older who live in private households in each participant country, regardless of their nationality, citizenship or language. The ESS allows for cross-national and cross-sectional comparisons of crime-related issues such as the confidence in police services, worry about crime and crime victimization experience in the last 5 years. For instance, it includes the following variables of interest for crime analysts:

1. *"Have you or a member of your household been the victim of a burglary or assault in the last 5 years?"*
2. *"How safe do you – or would you – feel walking alone in this area after dark?"*
3. *"Using this card, please tell me on a score of 0-10 how much you personally trust each of the institutions I read out [...]": "[...] the legal system" and "[...] the police".*

These measures have previously been used to study victimization, perceived safety, trust in the police, and trust in the legal system [e.g., @hough2014; @hummelsheim2011; @kaariainen2007], but there are many other questions that may also be of interest for criminologists (e.g., racism, discrimination against immigrants, homophobia). The whole ESS questionnaire is available here: [https://www.europeansocialsurvey.org/docs/round8/fieldwork/source/ESS8_source_questionnaires.pdf](https://www.europeansocialsurvey.org/docs/round8/fieldwork/source/ESS8_source_questionnaires.pdf).

With regards to the sampling design, participant countries are responsible for producing their own national sampling designs following some common sampling principles. Namely, respondents must be selected following strict random probability techniques at every stage, sampling frames can be individuals, households or addresses, quota sampling is not allowed, and non-responding units cannot be replaced. Moreover, every country must select at least 1,500 effective respondents (or at least 800 in participant countries with less than 2 million citizens). As a consequence, countries with very different population sizes may select similar sample sizes, and all geographical levels below countries (e.g., regions, counties, cities) are not planned by the original sampling design and suffer from small sample sizes.


### Download European Social Survey data

ESS data can be downloaded from their website. But we can also download ESS data directly into our `R` system using the `essurvey` package developed by @cimentada2019. This package is designed to facilitate loading ESS survey data into `R`. It allows users to select the countries and years they are interested to analyze and load survey data directly in `R` If this is the first time we are using this package, we need to install first it by using the `install.packages()` function.

``` {r install packs, eval = FALSE}

install.packages("essurvey") # install essurvey package

```

Once it is installed, we can load the package into our `R` environment using the `library()` function.

```{r load essurvey}

library(essurvey) # load essurvey package

```

In order to access ESS data in `R`, first we need to create our own personal account in the ESS online portal. ESS users need to register only once, and after that they can have open access to ESS data as many times as they wish. In order to sign up for a ESS account, we need to access the ESS website and create a new account with our personal details: [https://www.europeansocialsurvey.org/user/new](https://www.europeansocialsurvey.org/user/new). Filling the online registration form takes less than two minutes, and once it is completed we will receive an email to confirm our registration process.

Once we are registered in the ESS platform, we can directly import all ESS data into `R`. In this exercise we will download and analyze data from the 8th edition of ESS, which was published in 2016. We use the function `set_email()` from `essurvey` to save our email (the email account registered in the ESS platform) as a new environment variable, and then run the `import_rounds()` function to load ESS data from all participant countries. This may take a few seconds.

```{r read essurvey real, include=FALSE}

set_email("david.builgil@manchester.ac.uk")

ess <- import_rounds(rounds = 8)

```

```{r read essurvey fake, eval=FALSE}

set_email("your_email@domain.com") #change by your email

ess <- import_rounds(rounds = 8)   #load ESS data

```

Now we have loaded the ESS data from `r nrow(ess)` across 20 different countries, and we can begin exploring and analyzing these survet data. If we want to see how the dataset looks like, we can use the `View()` function.


## Descriptive analyses

In this exemplar study we will analyze ESS data about trust in police services, following previous research conducted by @kaariainen2007, @staubli2017 and others. The variable name is `trstplc`, and it a Likert scale variable from 0 to 10, where 0 indicates the lowest level of trust, and 10 is the maximum value. We can begin by checking how this measure of trust in the police looks like. We will use the `summary()` function to obtain the summary statistics of this variable. First, however, we need to transform the class of this variable from 'haven_labelled' (i.e., SPSS variable handled in `R`) to numeric to facilitate handling the data in `R`. We transform the variable using the `mutate()` function from `dplyr` package [@wickham2020] and convert it into numeric using the `as_numeric()` function.

```{r explore trust police}

library(dplyr) # load dplyr package

ess <- ess %>%                          # set dataset to use
  mutate(trstplc = as.numeric(trstplc)) # transform variable to numeric

summary(ess$trstplc) # print summary statistics

```

We see that the average score of trust in police in Europe is `r round(mean(ess$trstplc, na.rm = T), 2)`, and the median value is `r median(ess$trstplc, na.rm = T)`. We can use the same `summary()` function to analyze the values of trust in other social institutions, such as the legal system (variable `trstlgl`), politicians (`trtplt`), political parties (`trtprt`), the country's parliament (`trstprl`) or the United Nations (`trstun`). On average, Europeans appear to have more trust in the police than in other key political and legal institutions. We can also see that `r sum(length(which(is.na(ess$trstplc))))` persons did not answer this question.

We can obtain some more detailed information about the citizens' trust in the police by counting the frequency of respondents that chose each score and creating a bar plot to visualize their distribution. We will use functions from the the packages `dplyr` and `ggplot2` [@wickham2020b] for this. More specifically, we use the `group_by()` function from `dplyr` to create groups of respondents based on their score of trust in police, and use the functions `summarize()` and `mutate()` from the same package to save the results in two columns showing the number (`n`) and proportion (`prop`) of respondents in each category. We save this new table in a new dataset called `trust_poli`.

```{r table trust police}

trust_poli <- ess %>%       # set dataset to use
  group_by(trstplc) %>%     # group by score of trust
  summarize(n = n()) %>%    # number of respondents per group
  mutate(prop = n / sum(n)) # proportion respondents per group

```

Then, we use the `ggplot()` and `geom_bar()` functions from `ggplot2` to create a bar graph of the number of responses per score of trust. Before plotting this visualization, however, we will run the function `theme_set(theme_minimal())` to set a basic, neat theme for all our plots.

```{r plot trust police}

library(ggplot2) # load ggplot2 package

theme_set(theme_minimal()) # set white theme for plots

ggplot(data = trust_poli,              # set dataset to use
       aes(x = trstplc, y = prop)) +   # set variables for x and y axis
  geom_bar(stat="identity") +          # plot bars representing values in data
  ggtitle("Trust in police in Europe") # change title

```

We see that only a few respondents have a low trust in the police, whereas most European citizens appear to trust their police forces quite a lot. This plot, nevertheless, is likely to hide internal heterogeneity between European regions and countries. Based on this bar graph alone, we do not have enough information to be able to know if residents in all participant countries have the similar levels of trust in the police, or whether respondents with very low or very high confidence in the police concentrate in some countries but not others.


### Recode measure of trust in the police

In this exemplar study, we are particularly interested in analyzing which regions in Europe have especially low or high levels of trust in the police in comparison to other European countries. One way to do this is by estimating the proportion of residents in each region who have a level of trust above the average in Europe. Our final estimates will show a value between 0 and 1 representing the proportion of residents have more trust in the police than the European average. For instance, a value of 0.6 in a given region would indicate that 60 percent of its residents have more trust in the police than the average of all European citizens. 

Thus, we need to recode our variable of interest. We will use the `mutate()` and `ifelse()` functions from `dplyr` to do so. Those respondents with a score above or equal to the European mean will be given a value `1`, whereas the others will be assigned a value of `0`. This will facilitate the interpretation of our results, but future research can explore producing estimates from the original 0-to-10 Likert scale. We will also delete all those respondents who did not answer this question (i.e., *NA*s).

``` {r recode trustpoli}

ess <- ess %>%           # set dataset to use
                         # if trust above or equal to mean, assign 1, otherwise 0
  mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
  filter(!is.na(trstplc))# delete NAs

```

We can use the `group_by()` and `summarize()` functions seen above to explore how our recoded variable looks like. We can see, for example, that `r length(which(ess$trstplc == 1))` out of `r length(which(!is.na(ess$trstplc)))` (i,e., `r round((length(which(ess$trstplc == 1)) / length(which(!is.na(ess$trstplc))))*100, 2)`% of participants) have more trust in the police than the average in Europe.


## Exploring spatial data: Coverage and sample sizes

As mentioned above, the ESS sampling design is planned to allow producing reliable direct estimates at the country level, but samples recorded at smaller scales (e.g., regions, cities) may be too small to allow producing direct estimates of adequate precision in all areas. We can check how big ESS sample sizes are in each region (variable `region`) using the functions `filter()`, `group_by()` and `summarize()` from `dplyr` to create a summary table in a new dataframe that we will call `sample_region`. Then, we can use the `summary()` function to print the summary statistics of area sample sizes.

```{r sample regions}

sample_region <- ess %>%      # set dataset to use
  filter(region != 99999) %>% # filter out NAs
  group_by(region) %>%        # categories based on region
  summarize(n = n())          # calculate sample size

summary(sample_region$n) # print summary statistics

```

The average sample size per region is `r round(mean(sample_region$n), 1)`, which is quite large but may be insufficient to produce reliable direct estimates in all areas. We can also see that there are areas with very small sample sizes (the minimum area sample size is `r min(sample_region$n)`), where we cannot simply rely of direct estimation techniques to generate small area estimates of adequate precision.


### Checking spatial scale reported in each country

We also need to consider that participant countries can decide whether they want to publish their recorded data at the level of NUTS-1, NUTS-2, NUTS-3 or smaller spatial scales. NUTS is the acronym of *'Nomenclature of Territorial Units for Statistics'*, and it refers to spatial scales used by the European Union and Eurostat (the statistical office of the European Union) for policy making and statistical reporting purposes. NUTS are basically a way to organize European countries in regions and subregions. In England, for example, NUTS-1 are statistical regions, NUTS-2 are counties (and groups of districts in London), and NUTS-3 are generally unitary authorities (some grouped). Whereas some participant countries publish their data at the level of NUTS-2, others decide to report information for NUTS-1 or NUTS-3 areas. We can check which level of aggregation is published by each participant country in the ESS website: [https://www.europeansocialsurvey.org/data/multilevel/guide/essreg.html](https://www.europeansocialsurvey.org/data/multilevel/guide/essreg.html). We can also run the following lines of code and print this information directly in `R`.

```{r geo scale, eval=FALSE}

ess %>%                        # set dataset to use
  group_by(regunit, cntry) %>% # group by spatial scale and country
  summarize(n = n())           # print sample size per country

```

We see that many counties publish data at the NUTS-2 level, but others participant countries publish their micro-data for NUTS-1 and NUTS-3 areas. We will aggregate all data at the NUTS-2 level and produce estimates of confidence in the police at this spatial scale (with the exception of Germany and the UK, who only publish data for NUTS-1).


### Converting spatial data into NUTS-2

In order to convert the spatial information provided by ESS into NUTS-2 geographies, we first need to load a lookup table that details which NUTS-3 areas are part of which NUTS-2 geographies. I have previously created and saved a lookup table in csv format in an open access Github repository, but similar tables are also available in other formats from the ESS platform: [https://www.europeansocialsurvey.org/data/multilevel/guide/bulk.html](https://www.europeansocialsurvey.org/data/multilevel/guide/bulk.html). In order to load the lookup table in `R`, we can use the `getURL()` function from `RCurl` [@cran2020] and `read.csv()`.

``` {r read lookup}

library(RCurl) # load RCurl package

# save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")

lookup <- read.csv(text = url_lookup) # load lookup table

```

Now, we can create a new column in the original ESS data that specifies the regions for which we aim to produce small area estimates of trust in the police. To do this, first, we merge the lookup table with the original ESS data using a `left_join()` function, and then we create a new column called `domain` which shows the NUTS-2 areas (or NUTS-1 in Germany and UK) for which we will produce estimates.

``` {r ess nuts3 in nuts2}

ess <- ess%>%                                 #set dataset to use
  left_join(lookup, 
            by = c("region" = "nuts3")) %>%   #merge lookup into ESS dataset
  rename(domain = nuts2) %>%                  #rename NUTS2 variable as domain
  mutate(domain = as.character(domain),       #convert NUTS2 into character
         domain = ifelse(is.na(domain), 
                         region, domain)) %>% #copy NUTS1 if there is no NUTS2
  filter(!(domain == 99999))                  #delete NAs

```

Now our data is clean and ready to be used to produce estimates of trust in the police at a regional level.


## Producing direct estimates

We can begin by producing direct estimates in each area. As discussed before, direct estimators make use of original survey data and survey weights to obtain design-unbiased estimates in each small area, but direct estimates may be too unreliabile in those areas with small sample sizes. We will produce direct estimates of the trust in police for European regions, but it is very likely than many estimates will not show adequate levels of precision. Model-based SAE approaches are needed when direct estimates are not precise enough.

In order to produce small area estimates, we will use the `sae` package [@molina2020]. We need to install it and load it into your `R` system.

``` {r load sae}

library(sae) # load sae package

```

Direct estimators take into account the population size in each area, and assume that survey weights adjust our sample size to the total population in each area. In other words, we need to know the number of residents in each region, and ensure that our survey weights adjust the area sample size to the population size.


### Download data about population sizes

I have previously downloaded the regional population sizes from Eurostat and uploaded a clean dataset onto a Github repository. Downloading data from sources of official statistics, such as Eurostat, usually means having to spend some time cleaning the data and selecting those variables that adjust to our research needs. For the purpose of this exercise, I have downloaded, cleaned and saved the data previously, but later we will also see how to load Eurostat data into our `R` environments.

``` {r read population}

# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")

pop <- read.csv(text = url_pop) # load population size

```

Now we have almost all information necessary to produce our direct estimates: *(a)* the variable of interest (column `trstplc` in `ess` dataset), *(b)* the area population size (`pop2014` in `pop` dataset), and spatial information that matches both datasets. Nevertheless, as introduced above, direct estimators also require the use of survey weights that adjust our sample size to the population size in each.


### Readjust ESS survey weights

The survey weights published by ESS are not designed to let respondents represent a specific number of citizens, but instead they were computed to adjust chances of selection of every unit in the sample to the population characteristics. You can find a detailed guide about ESS survey weights here: [https://www.europeansocialsurvey.org/docs/methodology/ESS_weighting_data_1.pdf](https://www.europeansocialsurvey.org/docs/methodology/ESS_weighting_data_1.pdf). We will readjust our survey weights so that weighted respondents coincide to known population totals, while maintaining the design of the original weights that adjust for sample selection bias and unequal population sizes across countries. We can do this by running the following lines of code:

``` {r compute weight}

ess_w_area <- ess %>%                      #set dataset to use
  filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
  group_by(domain) %>%                     #create groups by region
  summarise(w_sum = sum(pspwght * pweight))#sum weights per region

ess <- ess %>%                               #set dataset to use
  filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
  left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units 
  left_join(pop, by = "domain") %>%          #merge region population sizes
  mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
         weight = (weight * pop2016) / w_sum)#readjust weights to population size

```

After a few steps, we now have all necessary information to produce our direct estimates of confidence in the police. We use the `direct()` function from `sae` to produce direct estimates in each region. It will also produce the CV of each estimate, which will be used to assess the reliability of these direct estimates.

``` {r direct}

dir <- direct(y       = ess$trstplc,#set variable of interest
              dom     = ess$area,   #areas to produce estimates 
              sweight = ess$weight, #survey weghts
              domsize = pop[,2:3])  #population size

```


### Exploring direct estimates

Once we have produced our direct estimates of trust in the police, we can see how these look like by using the `View()` function. We can also obtain some summary statistics of our direct estimates using the `summary()` function introduced before.

``` {r summary dir}

summary(dir$Direct) # summary statistics of direct estimates

```

Our direct estimates show that, across all areas, the average percentage of residents who have more trust in the police than the European average is `r round(mean(dir$Direct), 2)*100`%, but there is a large variation across areas. For instance, the region with the smallest direct estimate indicates that only `r round(min(dir$Direct), 2)*100`% of residents have more trust in the police than the average in Europe, whereas there is one region where `r round(max(dir$Direct), 2)*100`% of residents have more trust in the police than the European average.

We do not know yet, however, how reliable are these direct estimates. You can explore the direct estimates' CV by viewing the dataset (`View()` function), but you can also print some summary statistics that will give us a hint of the level of reliability of our direct estimates.

``` {r summary cv}

summary(dir$CV) # summary statistics of CV

```

As you can see, the average CV of our direct estimates (`r round(mean(dir$CV), 2)`) is quite small according to SAE standards, but the maximum value of CV is `r round(max(dir$CV), 2)`, which indicates that the direct estimate in at least one area is highly unreliable. If we observe our direct estimates, we will see that `r length(which(dir$CV >= 15))` direct estimates have measures of CV larger than 15%, `r length(which(dir$CV >= 25))` of them have a CV larger than 25%, and `r length(which(dir$CV >= 50))` of them has a CV larger than 50%. These results are not the end of the world, but we need to do our best to improve the reliability of small area estimates in those areas where their CV show inadequate levels of reliability. We need model-based SAE.

For now, however, we will merge our direct estimates into the dataset of area-level information by using the `left_join()` function from `dplyr`.

``` {r copy dir into pop}

pop <- pop %>%
  left_join(dir, by = c("area" = "Domain"))

```


## Downloading area-level covariates

In order to estimate area-level models and produce model-based small area estimates of trust in the police, we need area-level covariates that are associated with our variable of interest. We may need to conduct some preliminary literature review to explore which contextual elements have been used to explain our variable of interest. For example, in our case, we can read key articles and book analyzing the contextual predictors of trust in the police in European countries, such as @jackson2013, @kaariainen2007 and @staubli2017, but also in other contexts (e.g., @cao2012). We can see, for instance, that researchers have used measured related to crime rates, education level, income and level of democracy, among others, to explain the spatial distribution of trust in the police. We will download and use data about some of these as covariates to estimate our area-level models and produce model-based small area estimates.

We can download various area-level covariates from Eurostat using the `eurostat` package [@lahti2020]. Eurostat is a very large data repository that publishes large datasets of social, econonomic and demographic information for European countries and regions. We can use the `search_eurostat()` function to search predefined key words associated with variables of interest for our study, such as *"education"* or *"offender"*. The function will return a list of all available datasets including our keywords, and we can then explore which of them are more suitable to use as covariates in our area-level models. For example, we may want to know if education levels and crime rates are somehow associated with the regional levels of trust in the police. I have done this search and found various variables of interest, but you can also try this at home and probably you will also find variables of interest to use in our model.

```{r find covariates, eval=FALSE}

library(eurostat) # load eurostat package

eurostat_edu <- search_eurostat("education") #search data about education
eurostat_cri <- search_eurostat("offender")  #search data about crime

```

We see, for example, that our search has found 1006 datasets that contain the word *"education"*, and 4 that contain the word *"offender"*. We will need to spend some time searching those covariates that are more suitable for our study. Once we know the codes of the datasets we are interested to use, we can use the `get_eurostat()` function to import these into our `R` system. For example, the dataset `edat_lfs_9918` includes information about the proportion of citizens between 15 and 64 that have a higher education degree in each region. We can download this dataset and see how it looks like:

```{r get higher edu, eval=FALSE}

he <- get_eurostat(id = "edat_lfs_9918") #download data from Eurostat

```

This is only one possible source of data that we may use to find covariates for our area-level models. The ESS website, for example also publishes interesting area-level covariates at the different spatial scales ([https://www.europeansocialsurvey.org/data/multilevel/guide/bulk.html](https://www.europeansocialsurvey.org/data/multilevel/guide/bulk.html)). In any case, we will need to spend some time wrangling and subsetting these sources data to make sure we can use them as covariates in our area-level SAE models. 

For the purpose of this exemplar study, I have previously searched for datasets of interest, downloaded and cleaned their data, and merged all covariates into a unique dataset. I have also used Multiple Imputation via bootstrap and predictive mean matching to impute some missing values in certain regions (see original codes used for this here: [https://github.com/davidbuilgil/SAE_chapter/blob/master/other_codes/codes_imputation.R](https://github.com/davidbuilgil/SAE_chapter/blob/master/other_codes/codes_imputation.R)). We can load the dataset of clean and ready-to-use covariates into `R` using the functions provided by `RCurl` package, but you can also spend some time trying to find better, more suitable covariates in the Eurostat website.

``` {r read covariates}

# save URL address as character value
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short_imp.csv")

covs <- read.csv(text = url_covs) # save covariates

pop <- pop %>%
  left_join(covs, by = "domain") # merge covariates with direct estimates

```

rate crimes * 10000

Describe variables like this:
- *BLA*: bla bla bla


## Fitting area-level models and predicting synthetic estimates

And we will also substract all independent variables from the mean and divide these by two standard deviations, as suggested by @gelman2008, which will allow us to obtain standardised coefficients not affected by the dimensions of each variable:


``` {r fit model}

model <- lm(Direct ~  fem_p_16  + gdp_eurhab_16 + robb_r_10 + 
                      burg_r_10 +  medage_16    + he_p_16, 
            data = pop)

```


``` {r model output, echo = FALSE}

library(xtable)

model.sc <- lm(scale(Direct) ~  scale(fem_p_16)  + scale(gdp_eurhab_16) + scale(robb_r_10) + 
                                scale(burg_r_10) +  scale(medage_16)    + scale(he_p_16), 
            data = pop)

names(model.sc$coefficients) <- c('(Intercept)' , 'Proportion females', 'GDP per person (€)', 
                                  'Robbery rate', 'Burglary rate'     , 'Median age',
                                  'Proportion HE')

model_tab <- xtable(summary(model.sc)$coef, digits=c(0, 2, 2, 1, 2), 
                    caption = "Area-level model of trust in the police (standardized coefficients)")

```


`r model_tab`

Check r squared

`summary(model)$r.squared` 

`r round(summary(model)$r.squared, 2)`

We can also predict the synthetic estimates from our model. Synthetic estimation is the umbrella term used to describe the group of SAE techniques that produce small area estimates by fitting a regression model with area-level direct estimates as the dependent variable and relevant area-level auxiliary information as covariates and then computing regression-based predictions (i.e., synthetic estimates). 

Regression-based synthetic estimates can be produced for all areas regardless of their sample size (also areas with zero sample sizes). However, these are not based on a direct measurement of the variable in each area and suffer from a high risk of producing biased small area estimates [@levy1979; @rao2015].

We use the `predict()` function to product synthetic estimates from our area-level linear model.

``` {r predict lm}

synthetic <- predict(model) # predict synthetic estimates

```

``` {r merge synth}

pop <- pop %>%
  cbind(synthetic)

```

## Producing EBLUP estimates

Using the same variables, we also fit our EBLUP (i.e., Empirical Best Linear Unbiased Predictor) model to produce model-based small area estimates.

The area-level EBLUP, which is based on the model developed by @fay1979, obtains an optimal combination of direct and regression-based synthetic estimates in each small area. The EBLUP combines both estimates in each area and gives more weight to the direct estimate when its sampling variance is small, while more weight is attached to the synthetic estimate when the direct estimate’s variance is larger. The EBLUP reduces the variance of direct estimates and the risk of bias of synthetic estimates by producing the optimal combination of these in each area.

We use the `eblupFH()` function from `sae` package.

``` {r EBLUP}

eblup <- eblupFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 + 
                           pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 + 
                           pop$he_p_16,
                  vardir  = pop$SD^2,
                  method  = "REML")

```

``` {r print eblup, eval=FALSE}

eblup$fit # print model results

```


We can get the EBLUP model results by using the `summary()` function. describe min, mean and max here

And finally we can merge the model-based small area estimates into our main dataset of area-level information. We use the `cbind()` function to merge the new columns.
	
``` {r merge eblups}

pop <- pop %>%
  cbind(eblup$eblup) %>% # merge data into main dataset
  rename(eblup = "eblup$eblup") # change name of column

```
	

### Mapping the confidence in police work in Europe


Now we will load a shapefile of combined NUTS regions for all European countries. We will use the `eurostat_geodata_60_2016` shapefile, which is already saved in the `eurostat()` package. This dataset contains spatial information for all NUTS regions across various spatial scales, which will enable us to recode the NUTS-3 data into NUTS-2 codes.


```{r load geojson, results="hide"}

library(sf) # load sf package

url_nuts <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/shapefile/nuts_ess8.geojson")
nuts <- st_read(url_nuts)

# download geojson
#nuts <- st_read("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/shapefile/nuts_ess8.geojson")

st_crs(nuts) <- 15752 # change CRS to ED79 (EPSG:4668 with transformation)

```

And the last step is to map our small area estimates in Europe. We can use the following codes to prepare our shapefile:

``` {r map estimates}

geodata <- nuts %>%
  rename("domain" = "NUTS_ID") %>%
  left_join(pop, by = "domain") %>%
  filter(!is.na(Direct))

```

And this to visualise our maps of Direct and EBLUP estimates.


``` {r map}

ggplot(data = geodata) + 
  ggtitle("Trust in the police (EBLUP estimates)") +
  geom_sf(aes(fill = eblup)) +
  theme_void() +
  scale_fill_viridis_c(option = "plasma")

```


## Computing the Mean Squared Error of EBLUP estimates

In SAE, each small area estimate needs to be accompanied by its estimated measure of uncertainty, which is frequently defined by the Mean Squared Error (MSE) or the Relative Root Mean Squared Error (RRMSE). The MSE is a measure of the estimate’s reliability and refers to the averaged squared error of the estimate. Hence, it represents the squared difference between the estimated value and what is measured. The MSE is always non-negative, and values closer to zero indicate a higher reliability of the small area estimate. The MSE accounts for both the variance of the estimates (i.e., spread of estimates from one sample to another) and their bias (i.e., distance between the averaged estimated value and the true value). The RRMSE is obtained by taking the square root of the MSE (i.e. the Root Mean Squared Error, RMSE) and dividing it by the corresponding small area estimate. The RRMSE is usually presented as a percentage. This allows for direct comparisons between the measures of reliability of estimates obtained from direct and indirect model-based SAE techniques.

The RRMSE can be used to examine which SAE method produces the most reliable estimates and which estimates suffer from inadequate reliability. SAE methods may produce reliable estimates in some areas and unreliable estimates in others. SAE standards tend to establish that “estimates with RRMSEs greater than 25% should be used with caution and estimates with RRMSEs greater than 50% are considered too unreliable for general use” [@commonwealth2015, p. 13].

The measure of uncertainty of direct estimates is defined by their Coefficient of Variation (CV), which is the corresponding measure to the RRMSE for unbiased estimators [@rao2015].

RRMSEs of model-based estimates can be estimated following analytical and bootstrap procedures. In this exemplar study we will produce the RRMSE of our estimates by following an analytical approach: we use the `mseFH()` function from `sae`. 

``` {r mse eblup}

eblup_mse <- mseFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 + 
                         pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 + 
                         pop$he_p_16,
                   vardir  = pop$SD^2,
                   method  = "REML")

```

And we will also merge this information into our main dataset:

``` {r rrmse}

pop <- pop %>%
  cbind(eblup_mse$mse) %>% # merge data
  rename(mse = "eblup_mse$mse") %>% # change name of column
  mutate(rrmse = (sqrt(mse) / eblup) * 100) # compute RRMSE from MSE


```
	

### Plotting the Mean Squared Error of EBLUP estimates

Finally, we will also analyse to what extent our EBLUP small area estimates are more reliable than the original direct estimates. We will plot the RRMSE of EBLUP estimates and the CV of direct estimates using the `ggplot2` package.

``` {r plot rrmse}

pop %>%
  arrange(desc(SampSize)) %>%
  ggplot() + 
  geom_line(aes(y = CV, x = 1:nrow(pop), color = "darkred")) + # create red line of direct estimates' CV
  geom_line(aes(y = rrmse, x = 1:nrow(pop), color="steelblue")) + # create blue line of EBLUP's RRMSE
  scale_color_discrete(name = "Legend", labels = c("CV (direct)", "RRMSE (EBLUP)")) +
  ggtitle("RRMSE of direct and EBLUP estimates (ordered by area sample size)") 

```

Our small area estimates are more reliable in all areas, and the increased precision is very large is some cases.

## Model diagnostics

Diagnostics of our EBLUP estimates are presented below to examine whether our estimates are biased by the models and to check the model’s validity.

We start by producing a scatter plot of direct estimates against the EBLUP estimates. Regarding that direct estimates are design-unbiased, we expect a high linear correlation between direct and model-based estimates.

``` {r plot dir vs eblup}

library(gridExtra)

# plot direct estimates against EBLUPs
dir_vs_EBLUP <- ggplot(pop, aes(x = Direct, y = eblup)) + 
                geom_point() +
                ggtitle("Direct versus EBLUP estimates") +
                theme(aspect.ratio = 1)

# plot direct estimates against synthetic estimates
dir_vs_synth <- ggplot(pop, aes(x=Direct, y=synthetic)) + 
                geom_point() +
                ggtitle("Direct versus synthetic estimates") +
                theme(aspect.ratio = 1)

grid.arrange(dir_vs_EBLUP, dir_vs_synth, ncol=2)

```

dir vs eblup
ρ = `r round(cor.test(pop$Direct, pop$eblup, method = "spearman")$estimate, 2)` (p-value < 0.0001)

dir vs synth
ρ = `r round(cor.test(pop$Direct, pop$synthetic, method = "spearman")$estimate, 2)` (p-value < 0.0001)

The scatter plot and the Spearman's rank correlation coefficient show a high linear association between our model-based estimates and the unbiased direct estimates, which shows that our model does not bias our final small area estimates.

We will do the same with the synthetic estimates produced directly from the model, just to see the extent to which model-based synthetic estimates may be biased by the model

The scatter plot shows that many estimates are likely to be affected by bias arising from the model.

We can also calculate the model standardised residuals and present the q-q plots of residuals in order to check their normality. 

# Final remarks

# Author bio

David Buil-Gil is a Research Fellow at the Department of Criminology of the University of Manchester, UK, and a member of the Cathie Marsh Institute for Social Research at this same university. His research interests cover small area estimation applications in criminology, environmental criminology, crime mapping, emotions about crime, crime reporting, new methods for data collection and open data.

# Acknowledgments

The author would like to thank Samuel H. Langton and Angelo Moretti for comments that greatly improved the manuscript.

# References