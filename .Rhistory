summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + hcide_r_10    +
robb_r_10  + burg_r_10   + vthft_r_10  + he_p_16       +
medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
rm(list = ls())
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
rm(list = ls())
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + hcide_r_10    +
robb_r_10  + burg_r_10   + vthft_r_10  + he_p_16       +
medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
essurvey
library(essurvey) # load essurvey package
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8)
library(dplyr) # load dplyr package
ess <- ess %>%                          # set dataset to use
mutate(trstplc = as.numeric(trstplc)) # transform variable to numeric
summary(ess$trstplc) # print summary statistics
trust_poli <- ess %>%       # set dataset to use
group_by(trstplc) %>%     # group by score of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
library(ggplot2) # load ggplot2 package
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli,              # set dataset to use
aes(x = trstplc, y = prop)) +   # set variables for x and y axis
geom_bar(stat="identity") +          # plot bars representing values in data
ggtitle("Trust in police in Europe") # change title
ess <- ess %>%           #set dataset to use
#if trust above or equal to mean, assign 1, if not 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc))#delete NAs
sample_region <- ess %>%      # set dataset to use
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on region
summarize(n = n())          # calculate sample size
summary(sample_region$n) # print summary statistics
ess %>%                        # set dataset to use
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl) # load RCurl package
# save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
lookup <- read.csv(text = url_lookup) # load lookup table
ess <- ess%>%                                #set dataset to use
left_join(lookup,
by = c("region" = "nuts3")) %>%  #merge lookup into ESS dataset
rename(domain = nuts2) %>%                 #rename NUTS2 variable as domain
mutate(domain = as.character(domain),      #convert NUTS2 into character
domain = ifelse(is.na(domain),
region, domain)) %>%#copy NUTS1 if there is no NUTS2
filter(!(domain == 99999))                 #delete NAs
library(sae) # load sae package
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
dir <- direct(y       = ess$trstplc,#set variable of interest
dom     = ess$area,   #areas to produce estimates
sweight = ess$weight, #survey weghts
domsize = pop[,2:3])  #population size
summary(dir$Direct) # summary statistics of direct estimates
summary(dir$CV) # summary statistics of CV
pop <- pop %>%
left_join(dir, by = c("area" = "Domain"))
library(eurostat) # load eurostat package
eurostat_edu <- search_eurostat("education") #search data about education
#save URL address as character value
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short_imp.csv")
covs <- read.csv(text = url_covs) #save covariates
pop <- pop %>%
left_join(covs, by = "domain") #merge covariates with our data
#estimate area-level linear model
model <- lm(Direct ~ fem_p_16  + gdp_eurhab_16 + robb_r_10 +
burg_r_10 +  medage_16    + he_p_16,
data = pop)
View(pop)
min(pop$gdp_eurhab_16)
max(pop$gdp_eurhab_16)
round(min(pop$fem_p_16), 2)
round(max(pop$fem_p_16), 2)
round(min(pop$robb_r_10), 2)
round(max(pop$robb_r_10), 2)
?scale
?eblupFH
summary(eblup)
eblup$fit
xtable(eblup$fit$estcoef)
library(xtable)
xtable(eblup$fit$estcoef)
eblup.sc <- eblupFH(formula = scale(pop$Direct)    ~ scale(pop$fem_p_16)  + scale(pop$gdp_eurhab_16) +
scale(pop$robb_r_10) + scale(pop$burg_r_10) + scale(pop$medage_16) +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
names(eblup.sc$fit$estcoef) <- c('(Intercept)' , 'Proportion females', 'GDP per person (€)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
eblup.sc$fit$estcoef
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
library(essurvey) # load essurvey package
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8)
library(dplyr) # load dplyr package
ess <- ess %>%                          # set dataset to use
mutate(trstplc = as.numeric(trstplc)) # transform variable to numeric
summary(ess$trstplc) # print summary statistics
trust_poli <- ess %>%       # set dataset to use
group_by(trstplc) %>%     # group by score of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
library(ggplot2) # load ggplot2 package
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli,              # set dataset to use
aes(x = trstplc, y = prop)) +   # set variables for x and y axis
geom_bar(stat="identity") +          # plot bars representing values in data
ggtitle("Trust in police in Europe") # change title
ess <- ess %>%           #set dataset to use
#if trust above or equal to mean, assign 1, if not 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc))#delete NAs
sample_region <- ess %>%      # set dataset to use
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on region
summarize(n = n())          # calculate sample size
summary(sample_region$n) # print summary statistics
ess %>%                        # set dataset to use
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl) # load RCurl package
lookup <- read.csv(text = url_lookup) # load lookup table
ess <- ess%>%                                #set dataset to use
left_join(lookup,
by = c("region" = "nuts3")) %>%  #merge lookup into ESS dataset
rename(domain = nuts2) %>%                 #rename NUTS2 variable as domain
mutate(domain = as.character(domain),      #convert NUTS2 into character
domain = ifelse(is.na(domain),
region, domain)) %>%#copy NUTS1 if there is no NUTS2
filter(!(domain == 99999))                 #delete NAs
library(sae) # load sae package
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
dir <- direct(y       = ess$trstplc,#set variable of interest
dom     = ess$area,   #areas to produce estimates
sweight = ess$weight, #survey weghts
domsize = pop[,2:3])  #population size
summary(dir$Direct) # summary statistics of direct estimates
summary(dir$CV) # summary statistics of CV
pop <- pop %>%                              # set dataset to use
left_join(dir, by = c("area" = "Domain")) # merge direct estimates
#save URL address as character value
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short_imp.csv")
covs <- read.csv(text = url_covs) #save covariates
pop <- pop %>%                   #set dataset to use
left_join(covs, by = "domain") #merge covariates with our data
#estimate area-level linear model
model <- lm(Direct ~ fem_p_16  + gdp_eurhab_16 + robb_r_10 +
burg_r_10 +  medage_16    + he_p_16,
data = pop)
library(xtable)
model.sc <- lm(scale(Direct) ~ scale(fem_p_16)  + scale(gdp_eurhab_16) + scale(robb_r_10) +
scale(burg_r_10) +  scale(medage_16)    + scale(he_p_16),
data = pop)
names(model.sc$coefficients) <- c('(Intercept)' , 'Proportion females', 'GDP per person (€)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
model_tab <- xtable(summary(model.sc)$coef, digits=c(0, 2, 2, 1, 2),
caption = "Area-level model of trust in the police (standardized coefficients)")
synthetic <- predict(model) # predict synthetic estimates
pop <- pop %>%     # set dataset to use
cbind(synthetic) # merge regression-based estimates
eblup <- eblupFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup.sc <- eblupFH(formula = scale(pop$Direct)    ~ scale(pop$fem_p_16)  + scale(pop$gdp_eurhab_16) +
scale(pop$robb_r_10) + scale(pop$burg_r_10) + scale(pop$medage_16) +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup.sc$fit
eblup.sc$fit$estcoef
eblup.sc <- eblupFH(formula = scale(pop$Direct)    ~ scale(pop$fem_p_16)  + scale(pop$gdp_eurhab_16) +
scale(pop$robb_r_10) + scale(pop$burg_r_10) + scale(pop$medage_16) +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup.sc$fit$estcoef
eblup$fit$estcoef
View(eblup.sc)
View(eblup$fit$estcoef)
View(eblup.sc$fit$estcoef)
eblup <- eblupFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup$fit # print model results
eblup.sc <- eblupFH(formula = scale(pop$Direct)    ~ scale(pop$fem_p_16)  + scale(pop$gdp_eurhab_16) +
scale(pop$robb_r_10) + scale(pop$burg_r_10) + scale(pop$medage_16) +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
View(eblup.sc$fit$estcoef)
View(eblup$fit$estcoef)
eblup <- eblupFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup$fit # print model results
model_tab
eblup$fit
summary(eblup$eblup)
100-(round(min(eblup$eblup),2)*100)
View(eblup)
round(mean(eblup$eblup), 2)
round(sd(eblup$eblup), 2)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(dplyr)
library(sf) # load sf package
# save URL address as character value
url_nuts <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/shapefile/nuts_ess8.geojson")
nuts <- st_read(url_nuts) # load shapefile
st_crs(nuts)
View(pop)
pop <- pop %>%                 # set dataset to use
cbind(eblup$eblup) %>%       # merge data into main dataset
rename(eblup = "eblup$eblup")# change name of column
geodata <- nuts %>%                 # set dataset to use
rename("domain" = "NUTS_ID") %>%  # rename column of NUTS
left_join(pop, by = "domain") %>% # merge our area-level estimates
filter(!is.na(eblup))             # remove all regions without an estimate
View(geodata)
ggplot(data = geodata) +                            # set shapefile to use
ggtitle("Trust in the police (EBLUP estimates)") +# change title
geom_sf(aes(fill = eblup)) +                      # draw map of EBLUP estimates
scale_fill_viridis_c(option = "plasma")
st_crs(nuts) <- 15752 # change CRS to ED79 (EPSG:4668 with transformation)
ggplot(data = geodata) +                            # set shapefile to use
ggtitle("Trust in the police (EBLUP estimates)") +# change title
geom_sf(aes(fill = eblup)) +                      # draw map of EBLUP estimates
scale_fill_viridis_c(option = "plasma")
ggplot(data = geodata) +                            # set shapefile to use
ggtitle("Trust in the police (EBLUP estimates)") +# change title
geom_sf(aes(fill = eblup)) +                      # draw map of EBLUP estimates
theme_void() +
scale_fill_viridis_c(option = "plasma")
View(imputed)
View(geodata)
pop <- pop %>%                             #set dataset to use
cbind(eblup_mse$mse) %>%                 #merge MSEs of EBLUP estimates
rename(mse = "eblup_mse$mse") %>%        #change name of column
mutate(rrmse = (sqrt(mse) / eblup) * 100)#compute RRMSE from MSE and EBLUP
pop$CV < pop$rrmse
View(pop)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(dplyr)
pop %>%                                #set dataset to use
arrange(SampSize) %>%          #order by area sample size
ggplot() +                           #generate a plot
geom_line(aes(y     = CV,            #plot line of CV of direct
x     = 1:nrow(pop),   #visualize one point per area
color = "darkred")) +  #plot with red line
geom_line(aes(y     = rrmse,         #plot line of RRMSE of EBLUPs
x     = 1:nrow(pop),   #visualize one point per area
color = "steelblue")) +#plot with blue line
scale_color_discrete(name = "Legend",#add legend and change title
labels = c("CV (direct)", "RRMSE (EBLUP)")) +
ggtitle("RRMSE of direct and EBLUP estimates (ordered by sample size)")
pop %>%                                #set dataset to use
arrange(SampSize) %>%          #order by area sample size
ggplot() +                           #generate a plot
geom_line(aes(y     = CV,            #plot line of CV of direct
x     = 1:nrow(pop),   #visualize one point per area
color = "darkred")) +  #plot with red line
geom_line(aes(y     = rrmse,         #plot line of RRMSE of EBLUPs
x     = 1:nrow(pop),   #visualize one point per area
color = "steelblue")) +#plot with blue line
scale_color_discrete(name = "Legend",#add legend and change title
labels = c("CV (direct)", "RRMSE (EBLUP)")) +
ggtitle(label    = "RRMSE of direct and EBLUP estimates",
subtitle = "Areas ordered by sample size")
set_email("david.builgil@manchester.ac.uk")
library(essurvey) # load essurvey package
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8)
table(ess$region)
length(ess$region == 99999)
sum(ess$region == 99999)
table(ess$area)
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8)
library(dplyr) #load dplyr package
ess <- ess %>%                          #set dataset to use
mutate(trstplc = as.numeric(trstplc)) #transform column to numeric
summary(ess$trstplc) #print summary statistics
trust_poli <- ess %>%      #set dataset to use
group_by(trstplc) %>%    #group by score of trust
summarize(n = n()) %>%   #number of respondents per group
mutate(prop = n / sum(n))#proportion respondents per group
library(ggplot2) #load ggplot2 package
theme_set(theme_minimal()) #set white theme for plots
ggplot(data = trust_poli,             #set dataset to use
aes(x = trstplc, y = prop)) +  #set variables for x and y axis
geom_bar(stat="identity") +         #plot bars showing values in data
ggtitle("Trust in police in Europe")#change title
ess <- ess %>%           #set dataset to use
#if trust above or equal to mean, assign 1, if not 0
mutate(trstplc = if_else(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc))#delete NAs
sample_region <- ess %>%     #set dataset to use
filter(region != 99999) %>%#filter out NAs
group_by(region) %>%       #categories based on region
summarize(n = n())         #calculate sample size
summary(sample_region$n) #print summary statistics
ess %>%                       #set dataset to use
group_by(regunit, cntry) %>%#group by spatial scale and country
summarize(n = n())          #print sample size per country
library(RCurl) #load RCurl package
#save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
#save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
lookup <- read.csv(text = url_lookup) #load lookup table
ess <- ess%>%                                #set dataset to use
left_join(lookup,
by = c("region" = "nuts3")) %>%  #merge lookup into ESS dataset
rename(domain = nuts2) %>%                 #rename NUTS2 variable as domain
mutate(domain = as.character(domain),      #convert NUTS2 into character
domain = if_else(is.na(domain),
region, domain))%>%#copy NUTS1 if there is no NUTS2
filter(!(domain == 99999))                 #delete NAs
library(sae) #load sae package
#save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) #load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas from pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas from pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
dir <- direct(y       = ess$trstplc,#set variable of interest
dom     = ess$area,   #areas to produce estimates
sweight = ess$weight, #survey weights
domsize = pop[,2:3])  #population size
View(ess_w_area)
View(pop)
dir <- direct(y       = ess$trstplc,#set variable of interest
dom     = ess$area,   #areas to produce estimates
sweight = ess$weight, #survey weights
domsize = pop[,2:3])  #population size
View(dir)
ess %>%
group_by(domain) %>%
summarise(sum = sum(weight))
View(pop)
ess$weight
