url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + loun_ths_16 + unra1524_16 + unraall_16    +
hcide_r_10 + robb_r_10   + burg_r_10   + vthft_r_10    +
he_p_16    + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + hcide_r_10    +
robb_r_10  + burg_r_10   + vthft_r_10  + he_p_16       +
medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + robb_r_10     +
burg_r_10  + vthft_r_10  + he_p_16     + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16    + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
unra1524_16 + unraall_16  + robb_r_10   + burg_r_10     +
vthft_r_10  + he_p_16     + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16    + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
unraall_16  + robb_r_10   + burg_r_10   + vthft_r_10    +
he_p_16     + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16  + popdens_16 + gdp_eurhab_16 + unraall_16 +
robb_r_10 + burg_r_10  + vthft_r_10    +
he_p_16   + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + robb_r_10     +
burg_r_10  + vthft_r_10  + he_p_16     + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16    + popdens_16 + netmig_r_16 + gdp_eurhab_16 +
unra1524_16 + unraall_16 + robb_r_10   + burg_r_10     +
vthft_r_10  + he_p_16    + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + hcide_r_10    +
robb_r_10  + burg_r_10   + vthft_r_10  + he_p_16       +
medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
#save URL address as character value
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short_imp.csv")
covs <- read.csv(text = url_covs) #save covariates
pop <- pop %>%
left_join(covs, by = "domain") #merge covariates with our data
View(pop)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + hcide_r_10    +
robb_r_10  + burg_r_10   + vthft_r_10  + he_p_16       +
medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
rm(list = ls())
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
rm(list = ls())
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
View(covs)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + unra1524_16 + unraall_16  + hcide_r_10    +
robb_r_10  + burg_r_10   + vthft_r_10  + he_p_16       +
medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
essurvey
library(essurvey) # load essurvey package
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8)
library(dplyr) # load dplyr package
ess <- ess %>%                          # set dataset to use
mutate(trstplc = as.numeric(trstplc)) # transform variable to numeric
summary(ess$trstplc) # print summary statistics
trust_poli <- ess %>%       # set dataset to use
group_by(trstplc) %>%     # group by score of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
library(ggplot2) # load ggplot2 package
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli,              # set dataset to use
aes(x = trstplc, y = prop)) +   # set variables for x and y axis
geom_bar(stat="identity") +          # plot bars representing values in data
ggtitle("Trust in police in Europe") # change title
ess <- ess %>%           #set dataset to use
#if trust above or equal to mean, assign 1, if not 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc))#delete NAs
sample_region <- ess %>%      # set dataset to use
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on region
summarize(n = n())          # calculate sample size
summary(sample_region$n) # print summary statistics
ess %>%                        # set dataset to use
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl) # load RCurl package
# save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
lookup <- read.csv(text = url_lookup) # load lookup table
ess <- ess%>%                                #set dataset to use
left_join(lookup,
by = c("region" = "nuts3")) %>%  #merge lookup into ESS dataset
rename(domain = nuts2) %>%                 #rename NUTS2 variable as domain
mutate(domain = as.character(domain),      #convert NUTS2 into character
domain = ifelse(is.na(domain),
region, domain)) %>%#copy NUTS1 if there is no NUTS2
filter(!(domain == 99999))                 #delete NAs
library(sae) # load sae package
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
dir <- direct(y       = ess$trstplc,#set variable of interest
dom     = ess$area,   #areas to produce estimates
sweight = ess$weight, #survey weghts
domsize = pop[,2:3])  #population size
summary(dir$Direct) # summary statistics of direct estimates
summary(dir$CV) # summary statistics of CV
pop <- pop %>%
left_join(dir, by = c("area" = "Domain"))
library(eurostat) # load eurostat package
eurostat_edu <- search_eurostat("education") #search data about education
#save URL address as character value
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short_imp.csv")
covs <- read.csv(text = url_covs) #save covariates
pop <- pop %>%
left_join(covs, by = "domain") #merge covariates with our data
#estimate area-level linear model
model <- lm(Direct ~ fem_p_16  + gdp_eurhab_16 + robb_r_10 +
burg_r_10 +  medage_16    + he_p_16,
data = pop)
View(pop)
min(pop$gdp_eurhab_16)
max(pop$gdp_eurhab_16)
round(min(pop$fem_p_16), 2)
round(max(pop$fem_p_16), 2)
round(min(pop$robb_r_10), 2)
round(max(pop$robb_r_10), 2)
?scale
?eblupFH
summary(eblup)
eblup$fit
xtable(eblup$fit$estcoef)
library(xtable)
xtable(eblup$fit$estcoef)
eblup.sc <- eblupFH(formula = scale(pop$Direct)    ~ scale(pop$fem_p_16)  + scale(pop$gdp_eurhab_16) +
scale(pop$robb_r_10) + scale(pop$burg_r_10) + scale(pop$medage_16) +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
names(eblup.sc$fit$estcoef) <- c('(Intercept)' , 'Proportion females', 'GDP per person (€)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
eblup.sc$fit$estcoef
