synthetic <- predict(model) # predict synthetic estimates
pop <- pop %>%
cbind(synthetic)
eblup <- eblupFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup$fit # print model results
summary(eblup$eblup) # obtain model results
pop <- pop %>%
cbind(eblup$eblup) %>% # merge data into main dataset
rename(eblup = "eblup$eblup") # change name of column
library(sf)
# download geojson
nuts <- st_read("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/shapefile/nuts_ess8.geojson")
st_crs(nuts) <- 15752 # change CRS to ED79 (EPSG:4668 with transformation)
geodata <- nuts %>%
rename("domain" = "NUTS_ID") %>%
left_join(pop, by = "domain") %>%
filter(!is.na(Direct))
ggplot(data = geodata) +
ggtitle("Trust in the police (EBLUP estimates)") +
geom_sf(aes(fill = eblup)) +
theme_void() +
scale_fill_viridis_c(option = "plasma")
eblup_mse <- mseFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
pop <- pop %>%
cbind(eblup_mse$mse) %>% # merge data
rename(mse = "eblup_mse$mse") %>% # change name of column
mutate(rrmse = (sqrt(mse) / eblup) * 100) # compute RRMSE from MSE
pop %>%
arrange(desc(SampSize)) %>%
ggplot() +
geom_line(aes(y = CV, x = 1:nrow(pop), color = "darkred")) + # create red line of direct estimates' CV
geom_line(aes(y = rrmse, x = 1:nrow(pop), color="steelblue")) + # create blue line of EBLUP's RRMSE
scale_color_discrete(name = "Legend", labels = c("CV (direct)", "RRMSE (EBLUP)")) +
ggtitle("RRMSE of direct and EBLUP estimates (ordered by area sample size)")
plot(pop$Direct, pop$eblup) # plot direct estimates against EBLUPs
cor.test(pop$Direct, pop$eblup, method = "spearman") # Spearman rank correlation
plot(pop$Direct, pop$synthetic) # plot synthetic estimates against direct estimtaes
cor.test(pop$Direct, pop$synthetic, method = "spearman") # Spearman rank correlation
load("~/GitHub/SAE_chapter/.RData")
install.packages("utf8x")
update.packages(ask = FALSE, checkBuilt = TRUE)
tinytex::tlmgr_update()
tinytex::tlmgr_update()
tinytex::tlmgr_update()
tinytex::reinstall_tinytex()
tinytex:::is_tinytex()
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
install.packages("rlang")
install.packages("rlang")
library(dplyr)
library(rlang)
library(dplyr)
install.packages("dplyr")
install.packages("rlang")
library(dplyr)
summary(ess$trstplc)
set_email("david.builgil@manchester.ac.uk")
library(essurvey)
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
summary(ess$trstplc)
summary(ess$trstplc)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
install.packages("essurvey")
library(essurvey)
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
summary(ess$trstplc)
trust_poli <- ess %>%
group_by(trstplc) %>%     # categories based on level of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli, aes(x = trstplc, y = prop)) + # set variables of interest
geom_bar(stat="identity") +                           # plot bar graph
ggtitle("Trust in police across European countries")  # change title
ess <- ess %>%
# if trust is above or equal to mean, 1, 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc)) # delete NAs
sample_region <- ess %>%
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on regions
summarize(n = n())          # calculate sample size
summary(sample_region$n)
ess %>%
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl)
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
lookup <- read.csv(text = url_lookup)
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
library(sae)
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/population.csv")
pop <- read.csv(text = url_pop)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2)) %>%          # reorder columns
filter(domain %in% ess$domain)           # filter out areas not present in ESS
ess_w_area <- ess %>%
filter(domain %in% pop$domain) %>%        # filter out areas not present in population dataset
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
filter(domain %in% pop$domain) %>%          # filter out areas not present in population dataset
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
dir <- direct(y       = ess$trstplc,
dom     = ess$area,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
summary(dir$Direct) # summary statistics of direct estimates
# produce boxplot of coefficients of variation
ggplot(dir, aes(x=Domain, y=CV)) +
geom_boxplot() +
ggtitle("Coefficient of Variation of direct estimates")
pop <- pop %>%
left_join(dir, by = c("area" = "Domain"))
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short.csv")
covs <- read.csv(text = url_covs)
pop <- pop %>%
left_join(covs, by = "domain") # merge covariates with direct estimates
pop %>%
dplyr::select(fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16) %>%
summarise_all(funs(sum(is.na(.))))
library(Hmisc)
fun_imput <- aregImpute(~ fem_p_16 + gdp_eurhab_16 + robb_r_10 + burg_r_10 + he_p_16 + medage_16,
data = pop, n.impute = 10)
imputed <- as.data.frame(impute.transcan(fun_imput,
imputation = 1,
rhsImp = "mean",
data = pop,
list.out = T))
pop <- pop %>%
dplyr::select(domain, area, pop2016, SampSize, Direct, SD, CV) %>%
cbind(imputed)
model <- lm(Direct ~  fem_p_16  + gdp_eurhab_16 + robb_r_10 +
burg_r_10 +  medage_16    + he_p_16,
data = pop)
library(papeR)
model.sc <- lm(scale(Direct) ~  scale(fem_p_16)  + scale(gdp_eurhab_16) + scale(robb_r_10) +
scale(burg_r_10) +  scale(medage_16)    + scale(he_p_16),
data = pop)
names(model.sc$coefficients) <- c('(Intercept)' , 'Proportion females', 'GDP per person (â‚¬)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
xtable(prettify(summary(model.sc)), caption = "Area-level model of trust in the police (standadized coefficients)")
synthetic <- predict(model) # predict synthetic estimates
pop <- pop %>%
cbind(synthetic)
eblup <- eblupFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
eblup$fit # print model results
summary(eblup$eblup) # obtain model results
pop <- pop %>%
cbind(eblup$eblup) %>% # merge data into main dataset
rename(eblup = "eblup$eblup") # change name of column
library(sf)
# download geojson
nuts <- st_read("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/shapefile/nuts_ess8.geojson")
# download geojson
nuts <- st_read("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/shapefile/nuts_ess8.geojson")
st_crs(nuts) <- 15752 # change CRS to ED79 (EPSG:4668 with transformation)
geodata <- nuts %>%
rename("domain" = "NUTS_ID") %>%
left_join(pop, by = "domain") %>%
filter(!is.na(Direct))
ggplot(data = geodata) +
ggtitle("Trust in the police (EBLUP estimates)") +
geom_sf(aes(fill = eblup)) +
theme_void() +
scale_fill_viridis_c(option = "plasma")
eblup_mse <- mseFH(formula = pop$Direct    ~ pop$fem_p_16  + pop$gdp_eurhab_16 +
pop$robb_r_10 + pop$burg_r_10 + pop$medage_16 +
pop$he_p_16,
vardir  = pop$SD^2,
method  = "REML")
pop <- pop %>%
cbind(eblup_mse$mse) %>% # merge data
rename(mse = "eblup_mse$mse") %>% # change name of column
mutate(rrmse = (sqrt(mse) / eblup) * 100) # compute RRMSE from MSE
pop %>%
arrange(desc(SampSize)) %>%
ggplot() +
geom_line(aes(y = CV, x = 1:nrow(pop), color = "darkred")) + # create red line of direct estimates' CV
geom_line(aes(y = rrmse, x = 1:nrow(pop), color="steelblue")) + # create blue line of EBLUP's RRMSE
scale_color_discrete(name = "Legend", labels = c("CV (direct)", "RRMSE (EBLUP)")) +
ggtitle("RRMSE of direct and EBLUP estimates (ordered by area sample size)")
plot(pop$Direct, pop$eblup) # plot direct estimates against EBLUPs
cor.test(pop$Direct, pop$eblup, method = "spearman") # Spearman rank correlation
plot(pop$Direct, pop$synthetic) # plot synthetic estimates against direct estimtaes
cor.test(pop$Direct, pop$synthetic, method = "spearman") # Spearman rank correlation
ess %>% summarise(trstplc)
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
ess %>% summarise(trstplc)
summary(ess$trstplc)
ess$trstplc
ess %>% summarise(trstplc)
summarise(ess$trstplc)
?summary
summary(ess$trstplc)
install.packages("haven")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
library(essurvey)
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
summary(ess$trstplc)
install.packages("codebook")
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
summary(ess$trstplc)
library(codebook)
summary(ess$trstplc)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
library(essurvey)
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
summary(ess$trstplc)
ess <- as.data.frame(ess)
summary(ess$trstplc)
ess %>%
dplyr::select(trstplc) %>%
summary_table(.)
fivenum(ess$trstplc)
summary(ess$trstplc)
library(psych)
describe(ess$trstplc)
describe(ess$trstplc, na.rm = TRUE)
?import_rounds
summary(ess$trstplc)
trust_poli <- ess %>%
group_by(trstplc) %>%     # categories based on level of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli, aes(x = trstplc, y = prop)) + # set variables of interest
geom_bar(stat="identity") +                           # plot bar graph
ggtitle("Trust in police across European countries")  # change title
ess <- ess %>%
mutate(trstplc = as.factor(trstplc))
summary(ess$trstplc)
ess <- ess %>%
mutate(trstplc = as.numeric(trstplc))
summary(ess$trstplc)
par(mfrow=c(1,2)) # set the plotting area into 1x2 array
par(mfrow=c(1,2)) # set the plotting area into 1x2 array
plot(pop$Direct, pop$eblup) # plot direct estimates against EBLUPs
plot(pop$Direct, pop$synthetic) # plot synthetic estimates against direct estimtaes
library(gridExtra)
library(memisc)
install.packages("memisc")
library(sjPlot)
install.packages("sjPlot")
?tab_model
library(sjPlot)
?tab_model
model.sc <- lm(scale(Direct) ~  scale(fem_p_16)  + scale(gdp_eurhab_16) + scale(robb_r_10) +
scale(burg_r_10) +  scale(medage_16)    + scale(he_p_16),
data = pop)
names(model.sc$coefficients) <- c('(Intercept)' , 'Proportion females', 'GDP per person (â‚¬)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
tab_model(model.sc, file="model_output.html", show.est = TRUE, show.ci = TRUE,
show.p = TRUE, show.r2 = TRUE,
title = "Area-level model of trust in the police (standadized coefficients)")# You have to save the table in html format
library(xtable)
print(xtable(summary(model.sc)),type='html')
model.sc %>%
tidy() %>%
kable(
caption = "Area-level model of trust in the police (standadized coefficients)",
col.names = c("Predictor", "Estimate", "SE", "t-value", "p-value"),
digits = c(0, 2, 3, 2, 3)
)
library(tidyr)
library(knitr)
model.sc <- lm(scale(Direct) ~  scale(fem_p_16)  + scale(gdp_eurhab_16) + scale(robb_r_10) +
scale(burg_r_10) +  scale(medage_16)    + scale(he_p_16),
data = pop)
names(model.sc$coefficients) <- c('(Intercept)' , 'Proportion females', 'GDP per person (â‚¬)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
model.sc %>%
tidy() %>%
kable(
caption = "Area-level model of trust in the police (standadized coefficients)",
col.names = c("Predictor", "Estimate", "SE", "t-value", "p-value"),
digits = c(0, 2, 3, 2, 3)
)
install.packages("kableExtra")
library(kableExtra)
model.sc <- lm(scale(Direct) ~  scale(fem_p_16)  + scale(gdp_eurhab_16) + scale(robb_r_10) +
scale(burg_r_10) +  scale(medage_16)    + scale(he_p_16),
data = pop)
names(model.sc$coefficients) <- c('(Intercept)' , 'Proportion females', 'GDP per person (â‚¬)',
'Robbery rate', 'Burglary rate'     , 'Median age',
'Proportion HE')
model.sc %>%
tidy() %>%
kable(
caption = "Area-level model of trust in the police (standadized coefficients)",
col.names = c("Predictor", "Estimate", "SE", "t-value", "p-value"),
digits = c(0, 2, 3, 2, 3)
)
model.sc %>%
kable(
caption = "Area-level model of trust in the police (standadized coefficients)",
col.names = c("Predictor", "Estimate", "SE", "t-value", "p-value"),
digits = c(0, 2, 3, 2, 3)
)
model.sc %>%
tidy() %>%
kable(
caption = "Area-level model of trust in the police (standadized coefficients)",
col.names = c("Predictor", "Estimate", "SE", "t-value", "p-value"),
digits = c(0, 2, 3, 2, 3)
)
model_tab <- xtable(prettify(summary(model.sc)), caption = "Area-level model of trust in the police (standadized coefficients)")
model_tab <- xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), caption = "Area-level model of trust in the police (standadized coefficients)")
model_tab <- xtable(summary(model.sc)$coef, digits=c(0, 2, 2, 1, 2), caption = "Area-level model of trust in the police (standadized coefficients)")
print(model_tab, type="html")
install.packages("stargazer")
library(stargazer)
stargazer(model_sc)
stargazer(model.sc)
model_tab
install.packages("htmltools")
pop2 <- pop %>%
cbind(predict(model))
View(pop2)
cor.test(pop$Direct, pop$eblup, method = "spearman")$cor
cor <- cor.test(pop$Direct, pop$eblup, method = "spearman")
cor$statistic
cor$estimate
cor$p.value
cor.test(pop$Direct, pop$eblup, method = "spearman") # Spearman rank correlation
cor.test(pop$Direct, pop$synthetic, method = "spearman") # Spearman rank correlation
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(dplyr)
library(essurvey) # load essurvey package
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8)
library(dplyr) # load dplyr package
ess <- ess %>%                          # set dataset to use
mutate(trstplc = as.numeric(trstplc)) # transform variable to numeric
summary(ess$trstplc) # print summary statistics
trust_poli <- ess %>%       # set dataset to use
group_by(trstplc) %>%     # group by score of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
library(ggplot2) # load ggplot2 package
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli,              # set dataset to use
aes(x = trstplc, y = prop)) +   # set variables for x and y axis
geom_bar(stat="identity") +          # plot bars representing values in data
ggtitle("Trust in police in Europe") # change title
ess <- ess %>%           # set dataset to use
# if trust above or equal to mean, assign 1, otherwise 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc))# delete NAs
sample_region <- ess %>%      # set dataset to use
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on region
summarize(n = n())          # calculate sample size
summary(sample_region$n) # print summary statistics
ess %>%                        # set dataset to use
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl) # load RCurl package
# save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
# save URL address as character value
url_lookup <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
lookup <- read.csv(text = url_lookup) # load lookup table
ess <- ess%>%                                 #set dataset to use
left_join(lookup,
by = c("region" = "nuts3")) %>%   #merge lookup into ESS dataset
rename(domain = nuts2) %>%                  #rename NUTS2 variable as domain
mutate(domain = as.character(domain),       #convert NUTS2 into character
domain = ifelse(is.na(domain),
region, domain)) %>% #copy NUTS1 if there is no NUTS2
filter(!(domain == 99999))                  #delete NAs
library(sae) # load sae package
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
dir <- direct(y       = ess$trstplc,#set variable of interest
dom     = ess$area,   #areas to produce estimates
sweight = ess$weight, #survey weghts
domsize = pop[,2:3])  #population size
summary(dir$Direct) # summary statistics of direct estimates
summary(dir$CV) # summary statistics of CV
pop <- pop %>%
left_join(dir, by = c("area" = "Domain"))
library(eurostat) # load eurostat package
eurostat_edu <- search_eurostat("education") #search data about education
he <- get_eurostat(id = "edat_lfs_9918") #download data from Eurostat
# save URL address as character value
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs_short_imp.csv")
covs <- read.csv(text = url_covs) # save covariates
pop <- pop %>%
left_join(covs, by = "domain") # merge covariates with direct estimates
View(pop)
eurostat_cri <- search_eurostat("offender")  #search data about crime
# Load packages required.
packages <- c("dplyr", "RCurl", "Hmisc")
lapply(packages, require, character.only = TRUE)
# Load all possible covariates.
url_covs <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = url_covs)
# Check number of missing values per variable.
covs %>%
summarise_all(funs(sum(is.na(.))))
# Fit multiple imputation model via Bootstrapping and Predictive Mean Matching.
fun_imput <- aregImpute(~ fem_p_16   + popdens_16  + netmig_r_16 + gdp_eurhab_16 +
pps_hab_16 + loun_ths_16 + unra1524_16 + unraall_16    +
hcide_r_10 + robb_r_10   + burg_r_10   + vthft_r_10    +
he_p_16    + medage_16,
data = covs, n.impute = 10)
# Check R Squared with which each missing variable could be predicted from the others.
fun_imput$rsq
# Replace missing data with the mean of the multiple imputations for each case.
imputed <- as.data.frame(impute.transcan(fun_imput,       imputation = 1,
rhsImp = "mean", data = covs,
list.out = T))
# Replace imputed values in main dataset.
covs <- covs %>%
dplyr::select(domain) %>%
cbind(imputed)
# Select covariates for SAE.
covs_short <- covs %>%
dplyr::select(domain, fem_p_16, gdp_eurhab_16, robb_r_10, burg_r_10, he_p_16, medage_16)
# Save all imputed covariates.
write.csv(covs, "data/covs_imp.csv")
# Save selected imputed covariates.
write.csv(covs_short, "data/covs_short_imp.csv")
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
# save URL address as character value
url_pop <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/popsize.csv")
pop <- read.csv(text = url_pop) # load population size
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess_w_area <- ess %>%                      #set dataset to use
filter(domain %in% pop$domain) %>%       #filter areas present in pop dataset
group_by(domain) %>%                     #create groups by region
summarise(w_sum = sum(pspwght * pweight))#sum weights per region
ess <- ess %>%                               #set dataset to use
filter(domain %in% pop$domain) %>%         #filter areas present in pop dataset
left_join(ess_w_area, by = "domain") %>%   #merge sum of weights with ESS units
left_join(pop, by = "domain") %>%          #merge region population sizes
mutate(weight = pspwght * pweight,         #create weights for cross-national analysis
weight = (weight * pop2016) / w_sum)#readjust weights to population size
