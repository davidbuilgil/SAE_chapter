URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/NUTS_lookup.csv")
lookup <- read.csv(text = URL)
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
library(sae)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2))              # reorder columns
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
dplyr::rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2))              # reorder columns
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/population.csv")
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2))              # reorder columns
pop <- pop %>%
filter(pop$domain %in% ess$domain) # filter out areas not present in ESS
ess <- ess %>%
filter(ess$domain %in% pop$domain) # filter out areas not present in population dataset
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
sam <- table(ess$domain)
sam <- ess %>%
group_by(domain)
sam <- ess %>%
group_by(domain) %>%
summarise(n = n())
View(sam)
popu <- ess %>%
group_by(dominan) %>%
summarise(n = n())
popu <- ess %>%
group_by(domain) %>%
summarise(n = n())
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,1:3],
replace = FALSE)
View(dir)
popu <- pop %>%
group_by(domain) %>%
summarise(n = n())
sam <- ess %>%
group_by(domain) %>%
summarise(n = n(),
pop_w = sum(weight)) %>%
left_join(pop, by = "domain") %>%
popu <- pop %>%
group_by(domain) %>%
summarise(n = n())
sam <- ess %>%
group_by(domain) %>%
summarise(n = n(),
pop_w = sum(weight)) %>%
left_join(pop, by = "domain")
View(sam)
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2))              # reorder columns
pop <- as.data.frame(pop)
pop <- pop %>%
filter(pop$domain %in% ess$domain) # filter out areas not present in ESS
ess <- ess %>%
filter(ess$domain %in% pop$domain) # filter out areas not present in population dataset
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
ess <- ess %>%
# if trust is above or equal to mean, 1, 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc)) # delete NAs
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/NUTS_lookup.csv")
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/NUTS_lookup.csv")
lookup <- read.csv(text = URL)
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/population.csv")
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/population.csv")
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2))              # reorder columns
pop <- as.data.frame(pop)
pop <- pop %>%
filter(pop$domain %in% ess$domain) # filter out areas not present in ESS
ess <- ess %>%
filter(ess$domain %in% pop$domain) # filter out areas not present in population dataset
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
ess <- ess %>%
# if trust is above or equal to mean, 1, 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc)) # delete NAs
sample_region <- ess %>%
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on regions
summarize(n = n())          # calculate sample size
sample_region <- ess %>%
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on regions
summarize(n = n())          # calculate sample size
summary(sample_region$n)
ess %>%
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2))              # reorder columns
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,1:3],
replace = FALSE)
View(dir)
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
ess <- ess %>%
# if trust is above or equal to mean, 1, 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc)) # delete NAs
sample_region <- ess %>%
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on regions
summarize(n = n())          # calculate sample size
ess %>%
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl)
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/NUTS_lookup.csv")
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/NUTS_lookup.csv")
lookup <- read.csv(text = URL)
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/population.csv")
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/population.csv")
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2)) %>%          # reorder columns
filter(pop$domain %in% ess$domain)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2)) %>%          # reorder columns
filter(domain %in% ess$domain)
ess <- ess %>%
filter(ess$domain %in% pop$domain) # filter out areas not present in population dataset
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
ess <- ess %>%
# if trust is above or equal to mean, 1, 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc)) # delete NAs
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2)) %>%          # reorder columns
filter(domain %in% ess$domain)           # filter out areas not present in ESS
ess <- ess %>%
filter(domain %in% pop$domain) # filter out areas not present in population dataset
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
View(pop)
dir <- direct(y       = ess$trstplc,
dom     = ess$domain,
sweight = ess$weight,
domsize = pop[,1:3],
replace = FALSE)
View(dir)
View(pop)
View(sam)
sam <- ess %>%
group_by(domain) %>%
summarise(n = n(),
pop_w = sum(weight)) %>%
left_join(pop, by = "domain")
class(ess)
class(pop)
dir <- direct(y       = ess$trstplc,
dom     = ess$area,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
View(dir)
summary(dir$Direct) # summary statistics of direct estimates
summary(dir$CV) # summary statistics of coefficient of variation
# produce boxplot of coefficients of variation
ggplot(dir, aes(x=Domain, y=CV)) +
geom_boxplot() +
ggtitle("Coefficient of Variation of direct estimates")
pop <- pop %>%
left_join(dir, by = c("area" = "Domain"))
View(pop)
library(eurostat)
edu <- search_eurostat("education")
View(edu)
crime <- search_eurostat("offender")
he <- get_eurostat(id = "edat_lfs_9918")
View(he)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
he <- get_eurostat(id = "edat_lfs_9918")
he <- he %>%
filter(age == "Y15-64" & time >= "2016-01-01" & time < "2017-01-01" &
sex == "T" & isced11 == "ED5-8" & citizen == "TOTAL") %>%
dplyr::select(geo, values) %>%
rename(he_p = values)
pop <- pop %>%
left_join(he, by = c("geo", "geo"))
View(he)
View(pop)
pop <- pop %>%
left_join(he, by = c("domain", "geo"))
pop <- pop %>%
left_join(he, by = c("domain" = "geo"))
View(pop)
crim <- get_eurostat(id = "crim_gen_reg")
crim <- crim %>%
filter(time >= "2010-01-01" & time < "2011-01-01")
crim_homi <- crim %>%
filter(iccs == 	"ICCS0101") %>%
dplyr::select(geo, values) %>%
rename(homi = values)
crim_robb <- crim %>%
filter(iccs == "ICCS0401") %>%
dplyr::select(geo, values) %>%
rename(robb = values)
crim_burg <- crim %>%
filter(iccs == "ICCS05012") %>%
dplyr::select(geo, values) %>%
rename(burg = values)
crim_thefmot <- crim %>%
filter(iccs == "ICCS050211") %>%
dplyr::select(geo, values) %>%
rename(theftmot = values)
pop <- Reduce(function(...) merge(..., by = 'geo', all.x = TRUE),
list(pop, crim_burg, crim_homi, crim_robb, crim_thefmot))
pop <- Reduce(function(...) merge(..., by = c('domain' = 'geo'), all.x = TRUE),
list(pop, crim_burg, crim_homi, crim_robb, crim_thefmot))
crim_homi <- crim %>%
filter(iccs == 	"ICCS0101") %>%
dplyr::select(geo, values) %>%
rename(homi = values) %>%
rename(domain = geo)
crim_homi <- crim %>%
filter(iccs == 	"ICCS0101") %>%
dplyr::select(geo, values) %>%
rename(homi = values) %>%
rename(domain = geo)
crim_robb <- crim %>%
filter(iccs == "ICCS0401") %>%
dplyr::select(geo, values) %>%
rename(robb = values) %>%
rename(domain = geo)
crim_burg <- crim %>%
filter(iccs == "ICCS05012") %>%
dplyr::select(geo, values) %>%
rename(burg = values) %>%
rename(domain = geo)
crim_thefmot <- crim %>%
filter(iccs == "ICCS050211") %>%
dplyr::select(geo, values) %>%
rename(theftmot = values) %>%
rename(domain = geo)
pop <- Reduce(function(...) merge(..., by = 'domain', all.x = TRUE),
list(pop, crim_burg, crim_homi, crim_robb, crim_thefmot))
View(pop)
gdp <- get_eurostat(id = "nama_10r_2gdp")
gdp <- get_eurostat(id = "nama_10r_2gdp")
gdp <- gdp %>%
filter(time >= "2016-01-01" & time < "2017-01-01" & unit == "EUR_HAB_EU") %>%
dplyr::select(geo, values) %>%
rename(gdp = values)
gdp <- gdp %>%
filter(time >= "2016-01-01" & time < "2017-01-01" & unit == "EUR_HAB_EU") %>%
dplyr::select(geo, values) %>%
rename(gdp = values) %>%
rename(domain = geo)
gdp <- get_eurostat(id = "nama_10r_2gdp")
gdp <- gdp %>%
filter(time >= "2016-01-01" & time < "2017-01-01" & unit == "EUR_HAB_EU") %>%
dplyr::select(geo, values) %>%
rename(gdp = values) %>%
rename(domain = geo)
pop <- pop %>%
left_join(gdp, by = "geo")
pop <- pop %>%
left_join(gdp, by = "domain")
unem <- get_eurostat(id = "lfst_r_lfu3rt")
unem <- unem %>%
filter(time >= "2016-01-01" & time < "2017-01-01" & sex == "T" & age == "Y15-74") %>%
dplyr::select(geo, values) %>%
rename(unem = values) %>%
rename(domain = geo)
pop <- pop %>%
left_join(unem, by = "domain")
demo <- get_eurostat(id = "demo_r_pjanind3")
demo <- get_eurostat(id = "demo_r_pjanind3")
demo <- demo %>%
filter(indic_de == "MEDAGEPOP" & time >= "2016-01-01" & time < "2017-01-01") %>%
dplyr::select(geo, values) %>%
rename(medage = values)
pop <- pop %>%
left_joi
demo <- demo %>%
filter(indic_de == "MEDAGEPOP" & time >= "2016-01-01" & time < "2017-01-01") %>%
dplyr::select(geo, values) %>%
rename(medage = values) %>%
rename(domain = geo)
demo <- get_eurostat(id = "demo_r_pjanind3")
demo <- demo %>%
filter(indic_de == "MEDAGEPOP" & time >= "2016-01-01" & time < "2017-01-01") %>%
dplyr::select(geo, values) %>%
rename(medage = values) %>%
rename(domain = geo)
pop <- pop %>%
left_join(demo, by = "domain")
sex <- get_eurostat(id = "demo_r_d2jan")
sex <- sex %>%
filter(sex == "F" & time >= "2016-01-01" & time < "2017-01-01" & age == "TOTAL") %>%
dplyr::select(geo, values) %>%
rename(fem = values) %>%
rename(domain = geo)
pop <- pop %>%
left_join(sex, by = "domain")
write.csv(pop, "covs.csv")
head(ess)
names(ess)
View(pop)
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/covs.csv")
covs <- read.csv(text = URL)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(sf)
library(tidyr)
library(dplyr)
library(essurvey)
set_email("david.builgil@manchester.ac.uk")
ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)
trust_poli <- ess %>%
group_by(trstplc) %>%     # categories based on level of trust
summarize(n = n()) %>%    # number of respondents per group
mutate(prop = n / sum(n)) # proportion respondents per group
theme_set(theme_minimal()) # set white theme for plots
ggplot(data = trust_poli, aes(x = trstplc, y = prop)) + # set variables of interest
geom_bar(stat="identity") +                           # plot bar graph
ggtitle("Trust in police across European countries")  # change title
ess <- ess %>%
# if trust is above or equal to mean, 1, 0
mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0)) %>%
filter(!is.na(trstplc)) # delete NAs
sample_region <- ess %>%
filter(region != 99999) %>% # filter out NAs
group_by(region) %>%        # categories based on regions
summarize(n = n())          # calculate sample size
summary(sample_region$n)
ess %>%
group_by(regunit, cntry) %>% # group by spatial scale and country
summarize(n = n())           # print sample size per country
library(RCurl)
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/NUTS_lookup.csv")
lookup <- read.csv(text = URL)
ess <- ess%>%
left_join(lookup, by = c("region" = "nuts3")) %>%          # merge lookup into ESS dataset
rename(domain = nuts2) %>%                                 # rename NUTS2 variable
mutate(domain = as.character(domain),                      # convert NUTS2 into character
domain = ifelse(is.na(domain), region, domain)) %>% # copy NUTS1 data if no NUTS2 information
filter(!(domain == 99999))                                 # delete NAs
library(sae)
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/population.csv")
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/population.csv")
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2)) %>%          # reorder columns
filter(domain %in% ess$domain)           # filter out areas not present in ESS
ess <- ess %>%
filter(domain %in% pop$domain) # filter out areas not present in population dataset
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
dir <- direct(y       = ess$trstplc,
dom     = ess$area,
sweight = ess$weight,
domsize = pop[,2:3],
replace = FALSE)
summary(dir$Direct) # summary statistics of direct estimates
# produce boxplot of coefficients of variation
ggplot(dir, aes(x=Domain, y=CV)) +
geom_boxplot() +
ggtitle("Coefficient of Variation of direct estimates")
pop <- pop %>%
left_join(dir, by = c("area" = "Domain"))
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/covs.csv")
covs <- read.csv(text = URL)
pop2 <- pop %>%
left_join(covs, by = "domain") # merge covariates with direct estimates
View(pop2)
pop <- pop %>%
left_join(covs, by = "domain") # merge covariates with direct estimates
pop %>%
summarise_all(funs(sum(is.na(.))))
View(pop)
URL <- getURL("https://raw.githubusercontent.com/davidbuilgil/SAE_chapter/master/data/population.csv")
pop <- read.csv(text = URL)
pop <- pop %>%
mutate(area = 1:n()) %>%                 # create numeric id value
rename("domain" = "X.U.FEFF.domain") %>% # rename region column name
subset(select = c(1, 3, 2)) %>%          # reorder columns
filter(domain %in% ess$domain)           # filter out areas not present in ESS
ess <- ess %>%
filter(domain %in% pop$domain) # filter out areas not present in population dataset
ess_w_area <- ess %>%
group_by(domain) %>%                      # create groups by region
summarise(w_sum = sum(pspwght * pweight)) # sum weights per region
ess <- ess %>%
left_join(ess_w_area, by = "domain") %>%    # merge sum of weights with ESS units
left_join(pop, by = "domain") %>%           # merge region population sizes
mutate(weight = pspwght * pweight,          # compute weights for cross-national analysis
weight = (weight * pop2016) / w_sum) # recalibrate weights to population sample size
