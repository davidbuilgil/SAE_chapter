---
title: "Small area estimation. A 1-hour introduction for criminologists and crime analysts"
author: |
  | David Buil-Gil
  | Department of Criminology, University of Manchester, UK
date: "06/05/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
bibliography: refs.bib
abstract: Victimisation surveys provide essential information about crimes known and
  unknown to police and are the main source of data to analyse emotions about crime
  and perceptions about policing. Nevertheless, samples recorded by available survey
  are not large enough to allow for direct estimates (i.e., weighted means and totals)
  of adequate precision at small geographical levels. Refined model-based small area estimation
  techniques may be used to increase the reliability of small area
  estimates of parameters of criminological interest produced from survey data. Model-based
  small area estimation is the term used to describe the group of methods designed
  to produce reliable model-based estimates of parameters of interest (and associated
  measures of reliability) for areas or domains for which only small or zero sample
  sizes are available. SAE may be of great value for the study of small areas in criminological
  research; to estimate the geographical distribution of crimes known and unknown
  to police and to produce detailed maps of crime-related perceptions and emotions.
  This is the reason why, in 2008, the US Panel to Review the Programs of the Bureau
  of Justice Statistics recommended the use of model-based SAE to produce subnational
  estimates of crime rates. This course will introduce theory and a step-by-step exemplar
  study in `R` to show the utility of small area estimation in criminological research.
  Model-based regional estimates of confidence in policing are produced from European
  Social Survey data.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r Rmarkdown_setup, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r load myData, include=FALSE}
load("myData.RData")
```

**Full reference:** Buil-Gil, D. (2020). Small area estimation. A 1-hour introduction for criminologists and crime analysts. In Crimina Research Centre for the Study and Prevention of Crime (Org.), *Talleres de criminologia en la investigacion criminologica*.

**Contact details:** David Buil-Gil. G18 Humanities Bridgeford Street Building, Cathie Marsh Institute for Social Research, University of Manchester. E-mail address: *david.builgil@manchester.ac.uk*

**ORCID IDs:** David Buil-Gil: 0000-0002-7549-6317.

**Note:** Fragments of this documents are obtained from @builgil2019.

# 1. Introduction 

Criminological research and evidence-based criminal policy are progressively drifting towards the study of small geographic areas to explain and develop strategies to tackle crime and disorder, reduce public worries about crime, and improve perceptions about the criminal justice system. Crime is known to be concentrated in micro places [@weisburd2015], emotional responses to fear of crime tend to arise from environmental cues [@doran2011; @solymosi2015], and public perceptions about police work vary between small geographic areas and are associated with neighbourhood conditions [@builgil2020; @jackson2013]. These are not only topics of major interest for contemporary criminological research, but also have very large effects on local communities and citizens’ well-being. Precise maps of their distribution at small geographical scales are needed to allow for better theoretical explanations and more efficient evidence-based policies.

In order to map crime rates at a detailed geographical scale and examine their micro-level distribution patterns, police-recorded crimes are the most used source of data. While police records allow for advanced statistical analyses and are used to design targeted evidence-based urban policies, police-recorded crimes are known to suffer from missing data and the ‘dark figure of crime’ is likely to be larger in some areas than others [@brantingham2018; @obrien1996; @xie2019]. Victims from suburban areas are less likely to report crimes to police than urban and rural residents [@hart2003], and the neighbourhood conditions such as economic disadvantage, concentration of immigrants, crime rates and social cohesion may affect the victims’ crime reporting rates in some neighbourhoods more than others [@baumer2002; @berg2013; @builgil2020; @goudriaan2006; @xie2019]. Therefore, novel statistical techniques are needed to account for crimes unknown to police in order to develop micro-level crime maps of increased precision. Surveys provide essential information to investigate crimes known and unknown to police and may be used to produce maps of crime with increased validity [@skogan1977]. Nevertheless, samples recorded by available surveys are not large enough to allow for direct estimates of adequate precision at small geographical levels. Refined model-based small area estimation techniques (henceforth, SAE) may be used to increase the reliability of small area estimates of parameters of criminological interest produced from survey data.

The emotional reactions of fear and worry about crime are affected by the characteristics of the immediate environment and the conditions of local areas, and therefore these are more common in some areas than others. Fear of crime episodes are more frequent under certain situational and social organisation circumstances [@castrotoledo2017], and this is the reason why @solymosi2015 argue that there is a need to “consider fear of crime events at the smallest possible scale to be able to un-erroneously associate them spatially with elements of the environment” [-@solymosi2015, p. 198]. Certain community characteristics and social processes, such as neighbourhood disorder, residential instability and racial composition, are used to explain the unequal geographical distribution of worry about crime at a neighbourhood level [@bruntonsmith2012; @bruntonsmith2011]. At a larger geographical scale, there is a large amount of evidence about the effect of the countries’ social and economic issues on the citizens’ anxiety-producing concerns about crime [@builgil2019b; @hummelsheim2011; @vauclair2017]. However, these macro-level conditions are known to vary also between the regions in each country and thus are likely to be reflected unequally in the regional distribution of emotions about crime. Maps of fear and worry about crime at a neighbourhood and regional level are needed to better understand their explanatory mechanisms at the different scales and design targeted policies for their reduction. Similarly, perceptions about police work are influenced by neighbourhood conditions that affect some communities more than others [@jackson2013; @sampson1998].

Crime-related perceptions and emotions (i.e., worry about crime, perceptions of disorder, perceptions about police services and related constructs) are mainly recorded by social and victimisation surveys, such as the Crime Survey for England and Wales (CSEW) and the National Crime Victimization Survey (NCVS) in the US. Surveys are also needed to account for the dark figure of crime when producing crime maps. However, victimisation surveys are usually designed to allow for precise direct estimates of target parameters only for large geographical scales (e.g., states, regions, counties), while small geographic areas are unplanned domains and suffer from small (and zero) sample sizes that do not allow producing direct estimates of adequate precision. In this context, ‘unplanned domains’ refer to areas that were not identified at the sampling design stage (i.e., areas in which sample sizes cannot be controlled and where direct estimates are likely to be imprecise).
Model-based SAE techniques may be used in criminological research to produce reliable small area estimates of crime rates, emotions about crime and perceptions about the police, among other variables of criminological interest. SAE is the term used to refer to those techniques designed to produce reliable estimates of characteristics of interest (e.g., means, totals) for areas or domains for which only small or no samples are available [@pfeffermann2013; @rao2015].

This course will introduce theory and a step-by-step exemplar study in `R` [@R2020] to show the utility of small area estimation in criminological research. Model-based regional estimates of confidence in policing are produced from European Social Survey data.

# 2. SAE applications to crimological research

SAE may be of great value for the study of small areas in criminological research: to estimate the geographical distribution of crimes known and unknown to police and to produce detailed maps of crime-related perceptions and emotions. This is the reason why, in 2008, the US Panel to Review the Programs of the Bureau of Justice Statistics (BJS) recommended the use of model-based SAE to produce subnational estimates of crime rates: “BJS should investigate the use of modelling NCVS data to construct and disseminate subnational estimates of crime and victimization rates” [@groves2008, p. 8]. This work was started by Robert E. Fay and colleagues at the BJS to produce estimates of victimisation rates for states and large counties in the US [@fay2012; @fay2015]. The need for the incorporation of SAE to increase the reliability of subnational crime estimates has also been acknowledged by other governmental agencies for official statistics, such as the Australian Bureau of Statistics [@tanton2001], Statistics Netherlands [@buelens2009] and the Italian National Institute of Statistics [@dalo2012]. In the UK, the Government Statistical Service and the Office for National Statistics (ONS) have incorporated the use of SAE to produce estimates of income, health, housing affordability, unemployment and deprivation, but −to the extent of my knowledge− these agencies have not yet applied model-based SAE to criminological data.

Although SAE techniques have shown to be a very valuable tool to map social issues recorded by surveys, such as poverty and unemployment [e.g., @molina2010; @pratesi2016], and there is a clear need for their use in criminology, there has not been yet a detailed, unified examination of their applicability to analyse criminological data. Moreover, SAE techniques have been rarely applied in criminological research, and these may provide essential information to develop theoretical explanations of the effect of space on crime and crime perceptions. 

A large group of researchers have used different regression-based synthetic estimators to produce small area estimates in criminology. Although regression-based synthetic estimates can be produced for all areas (also for domains with zero and one sample sizes), these are known to suffer from a high risk of producing biased estimates due to model misspecification [@levy1979; @rao2015]. @hser1998 fitted an area-level logistic regression to predict synthetic estimates of drug use among arrestees in 185 American cities. @brugal1999 used an area-level log-linear model with interactions and produced regression-based synthetic estimates of the prevalence of addiction to opioids in the neighbourhoods of Barcelona. @tanton2001 used different area-level linear regression modelling approaches to produce synthetic estimates of victimisation rates at a local and regional level in Australia. @magnusson2011 made use of linear and logistic generalised regression models to estimate crime rates at a municipality and county level in Sweden. @whitworth2012 used multilevel modelling to produce synthetic estimates of fear of crime at neighbourhood level in England and Wales, and @taylor2013 used a similar approach to estimate perceived antisocial behaviour at the local and neighbourhood level in England and Wales. @wheeler2017 used multilevel and spatial models to predict synthetic estimates of attitudes towards police services at the neighbourhood level in an American city.

Others have used the basic unit-level or area-level SAE models, or the temporal extensions of the area-level EBLUP, to produce estimates of crime rates. @buelens2009 used the area-level EBLUP based on Fay-Herriot model [@fay1979] to produce estimates of victimisation rates in police zones in the Netherlands. Fay and colleagues developed the area-level dynamic SAE model, which is an extension of the temporal model developed by @rao1994, and produced estimates of crime rates in states and large counties in the US [@fay2012; @fay2015]. @dalo2012 made use of the basic unit-level and area-level EBLUP models to produce estimates of rates of violence against women at a regional level in Italy. @builgil2019b and @builgil2019c apply the spatial extension of the area-level EBLUP to produce estimates of worry about crime in Europe and perceived neighbourhood disorder in Manchester.

A third group of researchers make use of different Bayesian approaches to produce small area estimates of criminological data. @brakel2014 used a HB approach to estimate victimisation, perceived neighbourhood degeneration and contact with police at local level in Netherlands. @law2014 and @williams2019 made use of Bayesian spatiotemporal modelling to estimate crime rates at a neighbourhood level in the municipality of York (Canada) and confidence in police work in London, respectively.

Finally, other methodological approaches have also been used to produce small area estimates of crime and associated constructs. For example, @kongmuang2006 used spatial microsimulation to estimate crime at a ward level in Leeds, England. @mooney2018 examined the use of universal kriging to produce small area estimates of physical disorder for 1,826 block faces in four American cities (Detroit, New York, Philadelphia and San Jose).

# 3. Applying small area estimacion in criminology: Step-by-step example in `R`

In order to show the utility of small area estimation techniques in criminology, we present below an exemplar step-by-step study in `R`. Data from the European Social Survey will be accessed, downloaded, processed and analysed to produce regional estimates of confidence in the police.

## 3.1 European Social Survey

The European Social Survey is a biannual cross-national survey that has been conducted across 34 European countries since 2001. This survey measures attitudes, beliefs and behaviour patterns and allows for cross-national comparisons in Europe. The ESS questionnaire includes questions about victimisation, perceptions about police services, fear of crime and worry about crime, which may be of interest in criminological research.

European Social Survey samples are designed to be representative of all individuals aged 15 and over living in private households in each participant country, regardless of their nationality, citizenship or language. ESS participant countries are responsible for producing their national sample designs within common sampling principles; this is the reason why countries with different population sizes have similar sample sizes. These common sampling principles state that respondents need to be selected by strict random probability methods at every stage, sampling frames may be individuals, households or addresses, every country must sample a minimum of 1,500 effective respondents (800 in countries with populations smaller than 2 million), quota sampling is not permitted, and substitution of non-responding units is not permitted. In most countries, all geographical levels below country level (e.g., regions, counties, cities) are unplanned domains and suffer from small sample sizes.

## 3.2 Download European Social Survey data

The first step to automate the download of European Social Survey data is to sign up in their online portal. You will need to access this URL and sign up as a new user with your personal information: [https://www.europeansocialsurvey.org/user/new](https://www.europeansocialsurvey.org/user/new).

In order to automate the download of European Social Survey data into the `R` system, @cimentada2019 have developed the `essurvey` package, which allows users to load all survey edition across many countries without the need to access the European Social Survey website. You may need to install this package by using the `install.packages()` function.

``` {r install packs, eval = FALSE}

install.packages("essurvey") 

```

Once it is installed, you can load the package into your `R` system by using the `library()` function.

```{r load essurvey}

library(essurvey)

```

Please, do the same with the following packages: `ggplot2` [@wickham2020b], `dplyr` [@wickham2020], `tidyr` [@wickham2020c], `spdep` [@bivand2019], `stats` [@R2020b] and `ggpubr` [@kassambara2020]. Install these in your computer and load them into your `R` system. We will use them later today.

```{r load other packages}

library(ggplot2)
library(dplyr)
library(tidyr)
library(spdep)
library(stats)
library(ggpubr)

```

Once we have installed and loaded all necessary packages, we can import European Social Survey data. We will use data from its 8th edition, which was published in 2016. Use the function `set_email()` from `essurvey` package to save your email (the one registered in the European Social Survey platform) as en environment variable, and the `import_rounds()` function to load European Social Survey data. This may take a few seconds.

```{r read essurvey real, include=FALSE}

set_email("david.builgil@manchester.ac.uk")

ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)

```

```{r read essurvey fake, eval=FALSE}

set_email("your_email@gmail.com") # change by your email

ess <- import_rounds(rounds = 8, ess_email = NULL, format = NULL)

```

Now you have loaded the European Social Survey data and you can begin exploring and analysing it. If you want to see the data, you can use the `View()` function.

## 3.3 Explore European Social Survey data

The European Social Survey includes various questions that may be of interest for criminologists and crime analysts. You can have a look at the questionnaire here: [https://www.europeansocialsurvey.org/docs/round9/fieldwork/source/ESS9_source_questionnaires.pdf](https://www.europeansocialsurvey.org/docs/round9/fieldwork/source/ESS9_source_questionnaires.pdf).

Some of the European Social Survey variables that have been used in criminology are:

**1.-** "Have you or a member of your household been the victim of a burglary or assault in the last 5 years?"

**2.-** "How safe do you – or would you – feel walking alone in this area after dark?"

**3.-** "Using this card, please tell me on a score of 0-10 how much you personally trust each of the institutions I read out [...]"; "[...] the legal system" and "[...] the police".

These variables have been used to analyse victimisation, perceived safety, trust in the police and trust in the legal system, but there are many other variables that can also be of interest for criminologists (e.g., racism, discrimination, homophobia).

We will begin by checking how the variable of trust in police services looks like. We will use the `summary()` function to obtain the summary statistics of this variable.

```{r explore trust police}

summary(ess$trstplc)

```

We see that it a Likert scale variable from 0 to 10, where 0 indicates the lowest level of trust, and 10 the maximum. The average score is 6.4 and the median is 7. We can use the `dplyr` and `ggplot` packages to compute the frequency of respondents by score and create a bar plot. We use the `group_by()` function from `dplyr` to create groups based on the level of trust, and the functions `summarize()` and `mutate()` from `dplyr` to create two new columns representing the number of respondents per category and the proportion. We will save this new table in a new dataset called `trust_poli`. Before plotting the bar graph, we will run the function `theme_set(theme_minimal())` to use a basic, neat theme for all our plots.

```{r plot trust police}

trust_poli <- ess %>%
  group_by(trstplc) %>% # categories based on level of trust
  summarize(n = n()) %>% # number of respondents per group
  mutate(freq = n / sum(n)) # proportion respondents per group

theme_set(theme_minimal()) # set white theme for plots

ggplot(data = trust_poli, aes(x = trstplc, y = freq)) +
  geom_bar(stat="identity") +
  ggtitle("Trust in police across European countries")

```

In this exemplar study we will produce area-level estimates of the proportion of citizens who have a level of trust in police above the average in Europe. We will thus need to recode our variable using the `mutate()` and `ifelse()` functions: those respondents with a score above the mean will be given a value 1, whereas others will be assigned 0. This will facilitate the interpretation of our results, but future research can explore producing estimates from the original 0-to-10 Likert scale.

``` {r recode trustpoli}

ess <- ess %>%
  mutate(trstplc = ifelse(trstplc >= mean(trstplc, na.rm = T), 1, 0))

```

We can also use the same functions to explore of variables of interest in criminology, such as trust in the legal system...

```{r explore trust in legal system}

summary(ess$trstlgl)

trust_legals <- ess %>%
  group_by(trstlgl) %>% # categories based on level of trust
  summarize(n = n()) %>%
  mutate(freq = n / sum(n))

ggplot(data = trust_legals, aes(x = trstlgl, y = freq)) +
  geom_bar(stat="identity") +
  ggtitle("Trust in legal system across European countries")

```

... victimisation in the last 5 years (1 shows victimisation and 0 represents no victimisation)...

```{r explore victimization}

ess <- ess %>%
  mutate(crmvct = as.factor(crmvct), # convert into factor
         crmvct = recode(crmvct, "2" = "0")) # recode 2s as 0s

ess %>%
  group_by(crmvct) %>% # categories based on previous victimization
  summarize(n = n()) %>%
  mutate(freq = n / sum(n))

```

... or perceived safety after dark (1 is very safe and 4 is very unsafe)...

```{r explore fearcrime}

summary(ess$aesfdrk)

foc <- ess %>%
  group_by(aesfdrk) %>% # categories based on previous victimization
  summarize(n = n()) %>%
  mutate(freq = n / sum(n))

ggplot(data = foc, aes(x = aesfdrk, y = freq)) +
  geom_bar(stat="identity") +
  ggtitle("Perceived safety while walking alone after dark")

```

## 3.4 Explore spatial data within European Social Survey

Although the European Social Survey publishes data at the level of regions (i.e., variable `region`), its sampling design only allows to produce reliable direct estimates at the level of countries. We can check how big regional sample sizes are by running the following lines of code:

```{r sample regions}

sample_region <- ess %>%
  group_by(region) %>% # categories based on region
  summarize(n = n()) %>% # sample size
  mutate(freq = n / sum(n)) # proportion of total

summary(sample_region$n)

```

The average sample size per region is 161.4, which is quite large, but as you see we have areas with only one citizen sampled.

We should also consider that participant counties can decide whether they want to publish recorded data at the level of NUTS-1, NUTS-2, NUTS-3 or smaller scales. NUTS is the accronym of *Nomenclature of Territorial Units for Statistics*, and it refers of various spatial scales used by Eurostat and the European Union for policy making and statistical reporing purposes. It is basically a way to organize European countries in regions and subregions. In Spain, NUTS-1 are very large regions that aggregate more than one Autonomous Community, NUTS-2 areas are Autonomous Communities, and NUTS-3 are provinces. We can run the following lines of code to see which level of geography is used in each participant country.

```{r geo scale}

ess %>%
  group_by(regunit, cntry) %>% # categories based on previous victimization
  summarize(n = n()) %>%
  mutate(freq = n / sum(n))

```

We see that many counties publish data at the NUTS-2 level, but others publish micro-data for NUTS-1 and NUTS-3 areas. We will aggregate data at the NUTS-2 level and produce estimates of confidence in the police at this scale (with the exception of Germany and the UK, for which we will produce estimates at the NUTS-1 scale).

### 3.4.1 Convert spatial data into NUTS-2

We will create two spatial files of regions at the NUTS-2 and NUTS-3 levels and find the intersections between those polygons, which will be used to recode our spatial information to a NUTS-2 scale.

First we need to download the shapefile of geographic features of European NUTS-2 regions into our `R` system. To do that we will use the `eurostat` package [@lahti2020], which has been created by facilitate downloading data from Eurostat into `R`. As we have seen above, you may need to install it by using the `install.packages()` function. 

```{r eurostat library}

library(eurostat)

```

Now we will load a shapefile of combined NUTS regions for all European countries. We will use the `eurostat_geodata_60_2016` shapefile, which is already saved in the `eurostat()` package. This dataset contains spatial information for all NUTS regions across various spatial scales, which will enable us to recode the NUTS-3 data into NUTS-2 codes.

```{r load shapefile}

eurostat_geodata_60_2016 <- NULL

data("eurostat_geodata_60_2016", 
       envir = environment(),
       package = "eurostat")

```

We will create a shapefile for NUTS-2 regions by using the `filter()` code from `dplyr` and specifying that we are interested in level 2 regions. You may need to create a NULL value before filtering in NUTS-2 regions; if that is your case, you can run the line `NUTS2_geo <- NULL` beforehand.

```{r nuts2 shp}

#NUTS2_geo <- NULL

NUTS2_geo <- eurostat_geodata_60_2016 %>%
  filter(LEVL_CODE == 2) %>%
  mutate(nuts2.id = row_number(),
         nuts2.id = as.character(nuts2.id))

```

And we can do the same for NUTS-3 regions:

```{r nuts3 shp}

#NUTS3_geo <- NULL

NUTS3_geo <- eurostat_geodata_60_2016 %>%
  filter(LEVL_CODE == 3) %>%
  select(NUTS_ID)

```

Now we can use the `st_within()` function from `sf` [@pebesma2020] to seach NUTS-3 regions within NUTS-2 areas, copy this information in a new column in the `NUTS3_geo` dataset, and add the corresponding data from NUTS-2 in this file.

```{r points in poly}

NUTS3_geo <- NUTS3_geo %>%
  mutate(nuts2.id = as.data.frame(st_within(NUTS3_geo$geometry, NUTS2_geo$geometry))$col.id, # find NUTS3s within NUTS2s
         nuts2.id = as.character(nuts2.id)) %>% # transform new column into character
  left_join(as.data.frame(NUTS2_geo), by = "nuts2.id") %>% # merge with NUTS2 data
  select(NUTS_ID.x, NUTS_ID.y) %>% # select NUTS2 and NUTS3 id variables
  st_drop_geometry() # drop geographic information

```

Finally, we can the save the information about which NUTS-3 falls within each NUTS-2 area in the original `ess` dataset. In other words, we have recoded NUTS-3 areas into NUTS-2 geographies.

``` {r ess nuts3 in nuts2}

ess <- ess %>%
  left_join(NUTS3_geo, by = c("region" = "NUTS_ID.x")) %>% # merge NUTS3 data into ESS dataframe
  rename(NUTS2 = NUTS_ID.y) %>% # rename NUTS2 variable
  mutate(NUTS2 = ifelse(is.na(NUTS2), region, NUTS2)) %>% # copy NUTS1 data if there is no NUTS2
  filter(!(NUTS2 == 99999)) # delete NAs

```

Now our data is clean and we can begin producing estimates at the regional level.

## 3.5 Produce direct estimates

We will produce direct estimates based on the Horvitz-Thompson estimator. The Horvitz-Thompson estimator [@horvitz1952] is one of the most common approaches to produce direct estimates. It makes use of original survey data and survey weights to obtain design-unbiased estimates in each small area, but direct estimates suffer from high variance and unreliability in areas with small sample sizes and estimates cannot be produced in areas with zero samples. Thus, model-based SAE approaches are needed for areas where direct estimates are not precise enough.

In order to produce small area estimates, we will use the `sae` package. Install it and load it into your `R` system.

``` {r load sae}

library(sae)

```

The Horvitz-Thompson estimator takes into account the population size in each area, and assumes that survey weights adjust our sample to the total population. We need to know how many people live in each region, and we will use the `eurostat` package (loaded above) to download this data from Eurostat. The `demo_r_d2jan` dataset of Eurostat includes this type of information. We can use the `get_eurostat()` function to download this data into `R`. Then we will `filter()` out all that information that we are not interested to use.

``` {r popsize}

pop <- get_eurostat(id = "demo_r_d2jan")

pop <- pop %>%
  filter(time >= "2016-01-01" & time < "2017-01-01" & sex == "T" &
         age == "TOTAL")

```

We select only those NUTS regions that are present in our European Social Survey data by using the `filter()` function, select the geographic variable and the number of citizens living in each region by using the `select()` function, and create a new unique numeric value for each area by using the `mutate()` function. Then we reorder the columns and save this information as a data frame.

``` {r dataset popsize}

pop <- pop %>%
  filter(pop$geo %in% ess$NUTS2) %>% # filter out areas not present in ESS data
  dplyr::select(geo, values) %>% # select geo and values columns
  mutate(area = 1:n()) # create numeric id value

pop <- subset(pop, select = c(1, 3, 2)) # reorder columns

pop <- as.data.frame(pop) # save as data frame

```

Given that we will use data from Eurostat to fit our area-level models when producing area-level estimates, we will need to remove from the European Social Survey dataset all those regions/countries that are not part of Eurostat. Only by doing this we will be able to fit our models and produce reliable model-based small area estimates. We will also delete those respondents that did not answer the question of trust in police services.

``` {r remove areas ess}

ess <- ess %>%
  filter(ess$NUTS2 %in% pop$geo) %>% # filter out regions not included in Eurotsta
  filter(!is.na(trstplc)) %>% # filter out NAs in trust in police variable
  left_join(pop, by = c("NUTS2" = "geo")) # add NUTS-2 information

```

Now we have almost all information necessary to produce our direct estimates: the value of interest (*y*), the area population size (*N*), and spatial information that matches in both datasets. Nevertheless, as introduced above, the Horvitz-Thompson estimator also requires the use of survey weights that adjust our sample to the population size. Given that the weights published by the European Social Survey are not designed to let respondents represent a specific number of citizens, but instead they were computed to adjust the sample to the population characteristics, we will need to recalibrate the European Social Survey weights to the population sizes per region. We can do this by running the following lines of code:

``` {r compute weight}

# sum of weights per region
ess_w_area <- ess %>%
  group_by(area) %>% # create groups by region
  summarise(w_sum = sum(pspwght * pweight)) # sum weights per region

ess <- ess %>%
  left_join(ess_w_area, by = "area") %>% # merge sum of weights with ESS units 
  mutate(weight = pspwght * pweight, # compute weights for cross-national analysis
         weight = (weight * values) / w_sum) # recalibrate weights to population sample size


```

After a few steps, we now have all necessary information to produce our direct estimates of confidence in policing. We use the `direct()` function from `sae` to produce Horvitz-Thompson estimates in each region. It will also produce the Coefficient of Variation of each estimate, which will be used to assess the reliability of these direct estimates.

``` {r direct}

dir <- direct(y       = ess$trstplc,
              dom     = ess$area,
              sweight = ess$weight,
              domsize = pop[,2:3],
              replace = FALSE)


```

### 3.5.1 Explore direct estimates

Once our direct estimates have been produced, we can see how these look like by using some functions introduced above.

``` {r summary dir}

summary(dir$Direct) # summary statistics of direct estimates

summary(dir$CV) # summary statistics of coefficient of variation

# produce boxplot of coefficients of variation
ggplot(dir, aes(x=Domain, y=CV)) + 
  geom_boxplot() +
  ggtitle("Coefficient of Variation of direct estimates")

```

As you can see in the boxplot, the estimates of the majority of regions have a Coefficient of Variation smaller than 20%, which is a very good indicator of reliability of these estimates; but we also have a few regions with Coefficients of Variation larger than 25%. We can improve the accuracy of these estimates by using model-based small area estimation models.

For now, we can merge the direct estimates into the dataset of area-level information by using the `left_join()` function from `dplyr.

``` {r copy dir into pop}

pop <- pop %>%
  left_join(dir, by = c("area" = "Domain"))


```

# 3.6 Download covariates from Eurostat

In order to fit area-level models of confidence in police and produce area-level estimates, we will need area-level covariates that explain our variable of interest. We can download various area-level covariates from Eurostat using the `eurostat` package introduced before. Eurostat is a very large data repository that publishes large datasets of information from European countries and regions. We can use the `search_eurostat()` function to search key words associated with variables of interest for our study. For example, we may want to know if education levels and crime rates are somehow associated with the regional levels of confidence in the police, and thus we can search Eurostat datasets that include the words "attainment" and "offender".

I have done this search and found various variables of interest, but you can also try this at home and probably you will also find variables of interest for our models.

```{r find covariates}

edu <- search_eurostat("attainment")
crime <- search_eurostat("offender")

```

Below we see some lines of code to download and clean interesting area-level information about for our area-level models:

*(A)* Proportion of citizens between 15 and 64 with a higher education degree in 2016:

```{r get higher edu}

he <- get_eurostat(id = "edat_lfs_9918")

he <- he %>%
  filter(age == "Y15-64" & time >= "2016-01-01" & time < "2017-01-01" &
           sex == "T" & isced11 == "ED5-8" & citizen == "TOTAL") %>%
  dplyr::select(geo, values) %>%
  rename(he_p = values)

pop <- pop %>%
  left_join(he, by = c("geo", "geo"))

```

*(B)* Crimes (homicides, robberies, burglaries, theft of motor vehicle) in 2010 (no crime statistics have been published in Eurostat since then):

```{r get crime}

crim <- get_eurostat(id = "crim_gen_reg")

crim <- crim %>%
  filter(time >= "2010-01-01" & time < "2011-01-01")

crim_homi <- crim %>%
  filter(iccs == 	"ICCS0101") %>%
  dplyr::select(geo, values) %>%
  rename(homi = values)

crim_robb <- crim %>%
  filter(iccs == "ICCS0401") %>%
  dplyr::select(geo, values) %>%
  rename(robb = values)

crim_burg <- crim %>%
  filter(iccs == "ICCS05012") %>%
  dplyr::select(geo, values) %>%
  rename(burg = values)

crim_thefmot <- crim %>%
  filter(iccs == "ICCS050211") %>%
  dplyr::select(geo, values) %>%
  rename(theftmot = values)

pop <- Reduce(function(...) merge(..., by = 'geo', all.x = TRUE), 
              list(pop, crim_burg, crim_homi, crim_robb, crim_thefmot))

```

*(C)* Gross Domestic Product per capita in 2016:

``` {r get GDP}

gdp <- get_eurostat(id = "nama_10r_2gdp")

gdp <- gdp %>%
  filter(time >= "2016-01-01" & time < "2017-01-01" & unit == "EUR_HAB_EU") %>%
  dplyr::select(geo, values) %>%
  rename(gdp = values)

pop <- pop %>%
  left_join(gdp, by = "geo")

```

*(D)* Unemployment rate in 2016:

``` {r get unem}

unem <- get_eurostat(id = "lfst_r_lfu3rt")

unem <- unem %>%
  filter(time >= "2016-01-01" & time < "2017-01-01" & sex == "T" & age == "Y15-74") %>%
  dplyr::select(geo, values) %>%
  rename(unem = values)

pop <- pop %>%
  left_join(unem, by = "geo")

```

*(E)* Median age in 2016:

``` {r get age}

demo <- get_eurostat(id = "demo_r_pjanind3")

demo <- demo %>%
  filter(indic_de == "MEDAGEPOP" & time >= "2016-01-01" & time < "2017-01-01") %>%
  dplyr::select(geo, values) %>%
  rename(medage = values)

pop <- pop %>%
  left_join(demo, by = "geo")

```

*(F)* Number of females in 2016:

``` {r get sex}

sex <- get_eurostat(id = "demo_r_d2jan")

sex <- sex %>%
  filter(sex == "F" & time >= "2016-01-01" & time < "2017-01-01" & age == "TOTAL") %>%
  dplyr::select(geo, values) %>%
  rename(fem = values)

pop <- pop %>%
  left_join(sex, by = "geo")

```

*(G)* Number of citizens working for public administration and defence in 2015 (no data available for 2016):

``` {r}

jobs <- get_eurostat(id = "jvs_a_nace2")

jobs <- jobs %>%
  filter(time >= "2015-01-01" & time < "2016-01-01" & nace_r2 == "O" &
           indic_em == "JOBRATE" & isco08 == "TOTAL") %>%
  dplyr::select(geo, values) %>%
  rename(public = values)

pop <- pop %>%
  left_join(jobs, by = "geo")

```

*(H)* Rate of persons under risk of povery in 2016:

``` {r poverty}

poverty <- get_eurostat(id = "ilc_li41")
poverty2 <- get_eurostat(id = "ilc_peps11")

poverty <- poverty %>%
  filter(time >= "2016-01-01" & time < "2017-01-01" & unit == "PC") %>%
  dplyr::select(geo, values) %>%
  rename(pov_risk = values)
poverty2 <- poverty2 %>%
  filter(time >= "2016-01-01" & time < "2017-01-01" & unit == "PC") %>%
  dplyr::select(geo, values) %>%
  rename(povexc_risk = values)

pop <- pop %>%
  left_join(poverty, by = "geo")
pop <- pop %>%
  left_join(poverty2, by = "geo")

```

Once all our covariates are clean and ready to use, we can briefly explore them using the `dplyr` package. For instance, we may want to know the number of missing values in each covariate:
	
``` {r count nas}

pop %>%
  dplyr::select(he_p, burg, homi, robb, theftmot, gdp, unem, medage, fem, public, pov_risk, povexc_risk) %>%
  summarise_all(funs(sum(is.na(.))))

```

We can see our area-level dataset by using the `View()` function. Some of our these covariates are affected by many missing values; we will not use those area-level covariates with missing data in many areas.

Given that some of our covariates are totals, we may be interested in recoding them into rates (in the case of crimes) or proportions (in the case of females, for example). Moreover, we will use mean imputation to replace missing values by the average score of each variable. This method is not the best existing approach to impute missing values, but we will use this here given that it is a exemplar study with a learning outcome.

``` {r imput and recode rates}

pop <- pop %>%
  mutate(he_p = ifelse(is.na(he_p), mean(he_p, na.rm = T), he_p),
         burg_r = burg / values * 100,
         burg_r = ifelse(is.na(burg_r), mean(burg_r, na.rm = T), burg_r),
         homi_r = homi / values * 100,
         homi_r = ifelse(is.na(homi_r), mean(homi_r, na.rm = T), homi_r),
         robb_r = robb / values * 100,
         robb_r = ifelse(is.na(robb_r), mean(robb_r, na.rm = T), robb_r),
         theftmot_r = theftmot / values * 100,
         theftmot_r = ifelse(is.na(theftmot_r), mean(theftmot_r, na.rm = T), theftmot_r),
         gdp = ifelse(is.na(gdp), mean(gdp, na.rm = T), gdp),
         unem = ifelse(is.na(unem), mean(unem, na.rm = T), unem),
         fem_p = fem / values)

```
	
And we will also substract all independent variables from the mean and divide these by two standard deviations, as suggested by @gelman2008, which will allow us to obtain standardised coefficients not affected by the dimensions of each variable:

``` {r stand coef}

pop_st <- pop %>%
  mutate(he_p = (he_p - mean(he_p) / sd(he_p)*2),
         burg_r = (burg_r - mean(burg_r) / sd(burg_r)*2),
         homi_r = (homi_r - mean(homi_r) / sd(homi_r)*2),
         robb_r = (robb_r - mean(robb_r) / sd(robb_r)*2),
         theftmot_r = (theftmot_r - mean(theftmot_r) / sd(theftmot_r)*2),
         gdp = (gdp - mean(gdp) / sd(gdp)*2),
         unem = (unem - mean(unem) / sd(unem)*2),
         fem_p = (fem_p - mean(fem_p) / sd(fem_p)*2),
         medage = (medage - mean(medage) / sd(medage)*2))

```

## 3.7 Fit area-level models

Now our all variables (dependent and independent) are ready to be used in our models. We will fit an area-level linear model with the following covariates (higher education%, rate burglaries, rate homicies, rate robberies, rate theft motor vehicle, GDP, unemployment%, proportion females, and median age). We use the `lm()` function to fir the model, and we can see the results using the `summary()` function.

``` {r area-level lm}

model <- lm(Direct ~ he_p + burg_r + homi_r + robb_r + theftmot_r + gdp + unem + fem_p + medage, 
            pop_st) # fit lm model

summary(model) # print lm model results

```

We can also predict the synthetic estimates from our model. Synthetic estimation is the umbrella term used to describe the group of SAE techniques that produce small area estimates by fitting a regression model with area-level direct estimates as the dependent variable and relevant area-level auxiliary information as covariates and then computing regression-based predictions (i.e., synthetic estimates). Synthetic estimators may be based, for example, on area-level linear models [e.g., @brugal1999], logistic models [e.g., @hser1998], multilevel models [e.g., @taylor2013; @whitworth2012] and spatial models (e.g. Wheeler et al., 2017).

Regression-based synthetic estimates can be produced for all areas regardless of their sample size (also areas with zero sample sizes). However, these are not based on a direct measurement of the variable in each area and suffer from a high risk of producing biased small area estimates [@levy1979; @rao2015].

We use the `predict()` function to product synthetic estimates from our area-level linear model.

``` {r predict lm}

synthetic <- predict(model) # predict synthetic estimates

```

## 3.8 Produce EBLUP estimates

Using the same variables, we also fit our EBLUP (i.e., Empirical Best Linear Unbiased Predictor) model to produce model-based small area estimates.

The area-level EBLUP, which is based on the model developed by @fay1979, obtains an optimal combination of direct and regression-based synthetic estimates in each small area. The EBLUP combines both estimates in each area and gives more weight to the direct estimate when its sampling variance is small, while more weight is attached to the synthetic estimate when the direct estimate’s variance is larger. The EBLUP reduces the variance of direct estimates and the risk of bias of synthetic estimates by producing the optimal combination of these in each area.

We use the `eblupFH()` function from `sae` package.

``` {r EBLUP}

FHmodel <- eblupFH(formula = pop_st$Direct ~ pop_st$he_p + pop_st$burg_r + 
                     pop_st$homi_r + pop_st$robb_r + pop_st$theftmot_r + 
                     pop_st$gdp + pop_st$unem + pop_st$fem_p + 
                     pop_st$medage,
                   vardir  = pop_st$SD^2,
                   method  = "REML")

FHmodel$fit # print model results

```

We can get the EBLUP model results by using the `summary()` function.

``` {r EBLUP ests}

summary(FHmodel$eblup) # obtain summary statistics

```

And finally we can merge the model-based small area estimates into our main dataset of area-level information. We use the `cbind()` function to merge the new columns.
	
``` {r merge eblups}

pop_st <- pop_st %>%
  cbind(FHmodel$eblup) %>% # merge data into main dataset
  rename(eblup = "FHmodel$eblup") # change name of column

```
	
## 3.9 Produce Relative Root Mean Squared Error of EBLUP estimates

In SAE, each small area estimate needs to be accompanied by its estimated measure of uncertainty, which is frequently defined by the Mean Squared Error (MSE) or the Relative Root Mean Squared Error (RRMSE). The MSE is a measure of the estimate’s reliability and refers to the averaged squared error of the estimate. Hence, it represents the squared difference between the estimated value and what is measured. The MSE is always non-negative, and values closer to zero indicate a higher reliability of the small area estimate. The MSE accounts for both the variance of the estimates (i.e., spread of estimates from one sample to another) and their bias (i.e., distance between the averaged estimated value and the true value). The RRMSE is obtained by taking the square root of the MSE (i.e. the Root Mean Squared Error, RMSE) and dividing it by the corresponding small area estimate. The RRMSE is usually presented as a percentage. This allows for direct comparisons between the measures of reliability of estimates obtained from direct and indirect model-based SAE techniques.

The RRMSE can be used to examine which SAE method produces the most reliable estimates and which estimates suffer from inadequate reliability. SAE methods may produce reliable estimates in some areas and unreliable estimates in others. SAE standards tend to establish that “estimates with RRMSEs greater than 25% should be used with caution and estimates with RRMSEs greater than 50% are considered too unreliable for general use” [@commonwealth2015, p. 13].

The measure of uncertainty of direct estimates is defined by their Coefficient of Variation (CV), which is the corresponding measure to the RRMSE for unbiased estimators [@rao2015].

RRMSEs of model-based estimates can be estimated following analytical and bootstrap procedures. In this exemplar study we will produce the RRMSE of our estimates by following an analytical approach: we use the `mseFH()` function from `sae`. 

``` {r mse eblup}

mseFH <- mseFH(formula = pop_st$Direct ~ pop_st$he_p + pop_st$burg_r + 
                     pop_st$homi_r + pop_st$robb_r + pop_st$theftmot_r + 
                     pop_st$gdp + pop_st$unem + pop_st$fem_p + 
                     pop_st$medage,
                     vardir  = pop_st$SD^2,
                     method  = "REML")

```

And we will also merge this information into our main dataset:

``` {r rrmse}

pop_st <- pop_st %>%
  cbind(mseFH$mse) %>% # merge data
  rename(mse = "mseFH$mse") %>% # change name of column
  mutate(rrmse = (sqrt(mse) / eblup) * 100) # compute RRMSE from MSE


```
	
## 3.10 Model diagnostics

Diagnostics of our EBLUP estimates are presented below to examine whether our estimates are biased by the models and to check the model’s validity.

We start by producing a scatter plot of direct estimates against the EBLUP estimates. Regarding that direct estimates are design-unbiased, we expect a high linear correlation between direct and model-based estimates.

``` {r plot dir vs eblup}

plot(pop_st$Direct, pop_st$eblup) # plot direct estimates against EBLUPs
cor.test(pop_st$Direct, pop_st$eblup, method = "spearman") # Spearman rank correlation

```

The scatter plot and the Spearman's rank correlation coefficient show a high linear association between our model-based estimates and the unbiased direct estimates, which shows that our model does not bias our final small area estimates.

We will do the same with the synthetic estimates produced directly from the model, just to see the extent to which model-based synthetic estimates may be biased by the model

``` {r plot synth}

pop_st <- pop_st %>%
  cbind(synthetic)

plot(pop_st$synthetic, pop_st$eblup) # plot synthetic estimates against EBLUPs

```

The scatter plot shows that many estimates are likely to be affected by bias arising from the model.

We can calculate the model standardised residuals and present the q-q plots of residuals in order to check their normality. First we calculate the residuals.

``` {r residulals}

pop_st <- pop_st %>%
  mutate(res = (Direct - eblup) / sqrt(FHmodel$fit$refvar + SD^2))


```

And then we can plot the qqplot using the `qqnorm()` function.

``` {r plot residuals}

qqnorm(pop_st$res, asp = 1)

```

Most residuals are normally distributed, which is a good indicator.

## 3.11 Plotting RRMSE

Finally, we will also analyse to what extent our EBLUP small area estimates are more reliable than the original direct estimates. We will plot the RRMSE of EBLUP estimates and the CV of direct estimates using the `ggplot2` package.

``` {r plot rrmse}

ggplot(pop_st, aes(x=area)) + 
  geom_line(aes(y = CV), color = "darkred") + # create red line of direct estimates' CV
  geom_line(aes(y = rrmse), color="steelblue", linetype="twodash") # create blue line of EBLUP's RRMSE


```

Our small area estimates are more reliable in all areas, and the increased precision is very large is some cases.

## 3.12 Mapping confidence in police

And the last step is to map our small area estimates in Europe. We can use the following codes to prepare our shapefile:

``` {r map estimates}

geodata <- eurostat_geodata_60_2016 %>%
  left_join(pop_st, by = "geo") %>%
  filter(!is.na(Direct)) %>%
  dplyr::select(geo, Direct, eblup)


```

And this to visualise our maps of Direct and EBLUP estimates.

``` {r map}

Dirmap <- ggplot(data = geodata) + 
  ggtitle("Confidence in the police (Direct)") +
  geom_sf(aes(fill = Direct)) +
  theme_void() +
  scale_fill_viridis_c(option = "plasma")

FHmap <- ggplot(data = geodata) + 
  ggtitle("Confidence in the police (FH)") +
  geom_sf(aes(fill = eblup)) +
  theme_void() +
  scale_fill_viridis_c(option = "plasma")

ggarrange(Dirmap, FHmap)

```

# Author bio

David Buil-Gil is a Research Fellow at the Department of Criminology of the University of Manchester, UK, and a member of the Cathie Marsh Institute for Social Research at this same university. His research interests cover small area estimation applications in criminology, environmental criminology, crime mapping, emotions about crime, crime reporting, new methods for data collection and open data.

# References
